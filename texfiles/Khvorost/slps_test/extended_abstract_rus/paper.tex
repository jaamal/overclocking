\documentclass[11pt]{article}
\usepackage{cite}
\usepackage{a4}

\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath,amssymb,amsthm}

\usepackage{url}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{pictures}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% layout commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{thm}{Теорема}[section]
\newtheorem{lem}[thm]{Лемма}
\newtheorem{notation}[thm]{Наблюдение}

\theoremstyle{remark}
\newtheorem{definition}{Определение}[section]

\newcommand{\hm}[1]{#1\nobreak\discretionary{}{\hbox{\ensuremath{#1}}}{}}
\newcommand{\qqed}{\hfill$\Box$}

\renewenvironment{proof}{\trivlist\item[\hskip\labelsep{\bf Доказательство}.] }{\qqed\endtrivlist}
\newenvironment{proof1}{\trivlist\item[\hskip\labelsep{\bf Доказательство}] }{\qqed\endtrivlist}

\newcounter{examples}

\newenvironment{example}{\addtocounter{examples}{1}\trivlist\item[\hskip\labelsep{\bf
Пример \arabic{examples}.}] }{\endtrivlist}

\newcommand{\sectionnew}[1]{
    %TODO: eclipse hints that is better to use \centering command
    \addtocounter{section}{1}
    \setcounter{subsection}{0}
    \section*{{\centerline{\large\bf \S\arabic{section}. #1}}}
}

\newcommand{\subsectionnew}[1]{
    \addtocounter{subsection}{1}
    \paragraph*{\arabic{section}.\arabic{subsection}. #1}
}

\newcommand{\header}[1]{
    \noindent \textsc{#1}
}

\newcommand{\problem}[3]{\par\smallskip
    \header{Задача:} \textbf{#1} \newline
    \header{Вход:} #2 \newline
    \header{Выход:} #3
\smallskip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% math commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\substr}[2]{[#1\dots#2]}

\newcommand{\slp}[1]{\mathbb{#1}}

\newcommand{\tuple}[4]{
    #1_{#2}, #1_{#3}, \dots, #1_{#4}
}
\newcommand{\concat}[4]{
    #1_{#2} \cdot #1_{#3} \cdot \mbox{\dots} \cdot #1_{#4}
}

\DeclareMathOperator{\cnt}{count} \DeclareMathOperator{\splt}{\emph{split}}

\begin{document}

\title{
\begin{flushleft}
{\normalsize УДК 519.256}
\end{flushleft}
Эффективное сжатие данных\\ с помощью прямолинейных программ\thanks{Работа
выполнена при поддержке гранта РФФИ \No\,10-01-00793. }}
\author{И. С. Бурмистров \and А. В. Козлова \and Е. Б. Курпилянский \and А. А. Хворост}
\date{}
\maketitle

\begin{abstract}
Изучаются два алгоритма построения контекстно свободных грамматик, выводящих заданный текст. Первый алгоритм является модификацией
известного алгоритма Риттера и строит грамматику на основе AVL-деревьев, второй алгоритм использует декартовы деревья. Описываются
результаты экспериментов по сравнению эффективности этих двух алгоритмов и алгоритма Риттера на различных наборах данных и по сравнению
алгоритмы построения грамматик с алгоритмами из семейства алгоритмов Лемпеля-Зива по степени сжатия.
\end{abstract}

\sectionnew{Постановка задачи и структура работы}

Очевидна потребность в алгоритмах, способных эффективно обрабатывать большие объемы данных. Поскольку для хранения и передачи больших
объемов данных часто используются различные сжатые представления, один из возможных подходов к указанной проблеме состоит в разработке
алгоритмов, оперирующих непосредственно со сжатыми представлениями.

Ясно, что алгоритмы, работающие со сжатыми представлениями, существенным образом зависят от механизма сжатия. Имеется много разных методов
сжатого представления данных: коллаж-системы~\cite{collages}, представления с помощью антисловарей~\cite{antidictionaries}, прямолинейные
программы (кратко ПП)~\cite{SLPConstruction}, групповое кодирование~\cite{RunLengthEncoding} и~т.\,д. Сжатие текста с помощью
контекстно свободных грамматик (таких, как ПП) выделяется среди прочих методов двумя обстоятельствами. Во-первых, грамматики
обеспечивают хорошо структурированное сжатое представление, что удобно для последующей алгоритмической обработки.
Во-вторых, сжатие с помощью ПП полиномиально эквивалентно сжатию данных с помощью широко применяемых на практике
алгоритмов из семейства алгоритмов Лемпеля-Зива (таких, как, например, LZ78\,\cite{LZ78}, LZW\,\cite{LZW},
LZ77\,\cite{LZ77}). Полиномиальная эквивалентность здесь понимается в следующем смысле: существует полиномиальная
зависимость между размером ПП, выводящей данный текст $S$, и размером словаря, построенным алгоритмом Лемпеля-Зива для $S$, см.~\cite{SLPConstruction}.

Существует довольно большой класс задач, для которых разработаны алгоритмы со временем работы, полиномиальным
относительно размера сжатого представления текста с помощью ПП. К этому классу относятся, например, задачи
\textbf{Поиск образца в тексте}~\cite{PM_and_HD}, \textbf{Наибольшая общая подстрока}~\cite{LCSubstring}, считающая
версия задачи~\textbf{Поиск всех палиндромов}~\cite{LCSubstring}, некоторые версии задачи \textbf{Наибольшая общая
подпоследовательность}~\cite{LCS_P}. В то же время константы, которые скрываются за <<О большим>> в имеющихся оценках
сложности таких алгоритмов, как правило, очень велики. Кроме того, упомянутая выше полиномиальная связь между размером
ПП, выводящей данный текст $S$, и размером LZ77-словаря для $S$ еще не гарантирует, что ПП на практике обеспечивает
достаточно высокую степень сжатия. Поэтому вопрос о том, существуют ли методы сжатия, основанные на ПП и подходящие для
практического применения, требует дополнительного исследования. Данная работа задумана как шаг именно в этом
направлении. В ней рассматриваются два вопроса: насколько трудоемким является процесс построения ПП по данному тексту и
насколько высокий уровень сжатия может обеспечить ПП по сравнению с классическими алгоритмами.

Структура работы такова. В \S2 вводятся основные определения о строках и ПП. В \S3 представлены два алгоритма построения ПП. Первый
алгоритм является модифицированной версией алгоритма Риттера~\cite{SLPConstruction}. Второй алгоритм основан на новой структуре данных и
строит ПП, являющиеся декартовыми деревьями. В \S4 приводятся экспериментальные результаты по сравнению эффективности алгоритмов построения
ПП, а также по сравнению степени сжатия, достигаемой с помощью алгоритмов построения ПП и с помощью классических алгоритмов сжатия. Итоги
подводятся в \S5.

Часть результатов данной работы, относящаяся к модификации алгоритма Риттера, была представлена на конференции по
сжатию, передаче и обработке данных, проходившей в Палинуро, Италия, в июне 2011\,г. (CCP~2011, см.
\url{http://ccp2011.dia.unisa.it/CCP_2011/Home.html}), и анонсирована в \cite{BKh11}.

\sectionnew{Обозначения}

В работе рассматриваются строки над конечным алфавитом $\Sigma$. \emph{Длина} строки $S$ равна числу символов из $\Sigma$ в $S$ и
обозначается через $|S|$. \emph{Конкатенация} двух строк $S$ и $S'$ обозначается через $S\cdot S'$. \emph{Позицией} в строке $S$ называется
место между соседними символами. Мы нумеруем позиции произвольной строки $S$ слева направо, начиная с 1 и заканчивая $|S|-1$. Удобно ввести
также позицию 0, которая предшествует строке $S$, и позицию $|S|$, которая следует за $S$. Для строки $S$ и числа $i$ такого, что $0 \le i
< |S|$, обозначим через $S[i]$ символ, расположенный между позициями $i$ и $i+1$. Например, $S[0]$ -- это первый символ строки $S$.
Обозначим через $S\substr{\ell}{r}$, где $0\le \ell < r \le |S|$, подстроку строки $S$, которая начинается с позиции $\ell$ и заканчивается
в позиции $r$. Таким образом, $S\substr{\ell}{r} = S[\ell] \cdot S[\ell + 1] \cdot \mbox{\dots} \cdot S[r-1]$.

\emph{Прямолинейная программа} (ПП) $\slp{S}$ -- это последовательность присваиваний следующего вида:
$$\slp{S}_1 \to expr_1,\ \slp{S}_2\to expr_2,\ \dots,\ \slp{S}_n \to expr_n,$$
где $\slp{S}_i$ называются \emph{правилами}, а $expr_i$ называются \emph{выражениями}. Выражения бывают двух видов:
\begin{itemize}
\item $expr_i$ есть символ из $\Sigma$ (тогда правило $\slp{S}_i$ называется \emph{терминальным}),
\item $expr_i$ есть $\slp{S}_\ell \cdot \slp{S}_r$, где $\ell,r < i$ (тогда правило $\slp{S}_i$ называется \emph{нетерминальным}).
\end{itemize}

Таким образом, ПП -- это контекстно свободная грамматика в нормальной форме Хомского, порождающая в точности одну строку над алфавитом
$\Sigma$. Строку $S$, выводимую из ПП $\slp{S}$, будем называть \emph{текстом}. Для ПП $\slp{S}$, выводящей текст $S$, определим
\emph{дерево вывода} текста $S$ как дерево вывода грамматики $\slp{S}$. В дереве вывода мы отождествляем терминальные узлы с их родителями,
поэтому оно является двоичным. На рис.\,\ref{fibonacci_word_slp} представлено дерево вывода ПП $$ \slp{F}_0{\to} b,\ \slp{F}_1{\to} a,\
\slp{F}_2{\to} \slp{F}_1\cdot\slp{F}_0,\ \slp{F}_3{\to} \slp{F}_2\cdot\slp{F}_1,\ \slp{F}_4{\to} \slp{F}_3\cdot\slp{F}_2,\ \ \slp{F}_5{\to}
\slp{F}_4\cdot\slp{F}_3,\ \ \slp{F}_6{\to} \slp{F}_5\cdot\slp{F}_4,$$ выводящей 6-е слово Фибоначчи $abaababaabaab$.
\begin{figure}[hb]
    \begin{center}
        \begin{picture}(100,175)(120,10)
            \fibonacciwordslp
        \end{picture}
    \end{center}
    \caption{Прямолинейная программа, выводящая слово $abaababaabaab$}
    \label{fibonacci_word_slp}
\end{figure}
В этом примере слово длины 13 порождается  ПП из 7 правил. Аналогично, в общем случае $n$-e слово Фибоначчи может быть порождено ПП
$$\slp{F}_0\to b,\ \slp{F}_1\to a,\ \slp{F}_2\to \slp{F}_1\cdot\slp{F}_0,\ \slp{F}_3\to \slp{F}_2\cdot\slp{F}_1,\ \dots,\
\slp{F}_n\to \slp{F}_{n-1}\cdot\slp{F}_{n-2}$$ из $n+1$ правил. Поскольку длина $n$-го слова Фибоначчи есть $(n+1)$-e число Фибоначчи,
т.\,е.\ ближайшее целое к $\frac{\varphi^{n+1}}{\sqrt{5}}$, где $\varphi=\frac{1+\sqrt{5}}2$ (золотое сечение), мы видим, что для некоторых
текстов их сжатое представление с помощью ПП может обеспечить экспоненциальный выигрыш в пространстве.

В работе приняты следующие договоренности: все ПП обозначаются с помощью заглавных ажурных букв, например, $\slp{S}$. Каждое правило ПП
$\slp{S}$ (и каждый внутренний узел ее дерева вывода) обозначается аналогичной буквой с порядковым номером, например, $\slp{S}_i$.
\emph{Размер} ПП $\slp{S}$ полагается равным числу правил и обозначается через $|\slp{S}|$. \emph{Высота} узла двоичного дерева
определяется рекурсивно: высота листа полагается равной~0, а высота внутреннего узла полагается равной 1 + максимум из высот его сыновей.
Обозначим через $h(\slp{S}_i)$ высоту правила $\slp{S}_i$.

\emph{Конкатенацией} ПП $\slp{S}$ и $\slp{S}'$, выводящих тексты $S$ и $S'$ соответственно,  называется ПП $\slp{S}\cdot\slp{S}'$, которая
выводит текст $S\cdot S'$. Подчеркнем, что конкатенация ПП не есть однозначно определенная операция (в отличие от конкатенации строк):
сконструировать  из $\slp{S}$ и $\slp{S}'$ ПП, выводящую $S\cdot S'$, можно разными способами, и выбор конкретного способа обычно
определяется спецификой решаемой задачи.

\sectionnew{Алгоритмы построения прямолинейных программ}

\subsectionnew{Прямолинейные программы, факторизации и деревья.} Классическая формулировка задачи построения ПП
такова:

\problem{Построение ПП по данному тексту}{текст $S$;}{ПП $\slp{S}$, которая выводит $S$.}

Известно, что задача построения грамматики минимального размера для заданного текста является NP-трудной
\cite{SmallestCFG}. Поэтому представляют интерес эффективные приближенные алгоритмы. Один из подходов к быстрому
построению ПП отталкивается от факторизации текста. Если зафиксирована такая факторизация, то на каждом шаге алгоритм
строит ПП, которая выводит очередной фактор, а затем конкатенирует ПП, построенную на предыдущих шагах, с ПП, выводящей
текущий фактор. При таком подходе размер результирующей ПП зависит не только от размера исходного текста, но и от
способа факторизации. Следовательно, задача построения ПП может быть уточнена таким образом:

\problem{Построение ПП по данной факторизации текста}{текст $S$ и его факторизация $F_1, F_2, \dots, F_k$;}{ПП $\slp{S}$, которая выводит
$S$.}

Риттер \cite{SLPConstruction} в качестве исходной факторизации выбрал факторизацию, порождаемую алгоритмом LZ77. Это позволило установить
связь между размером ПП, выводящей данный текст $S$, и размером LZ77-словаря для $S$. Кроме того, благодаря свойствам LZ-факторизаций, ПП
для текущего фактора всегда является частью уже построенной на предыдущих шагах ПП, что радикально ускоряет вычисления.

\begin{definition}
\emph{LZ-факторизация} текста $S$ -- это последовательность строк $F_1, F_2, \dots, F_k$ такая, что $S = F_1 \cdot F_2
\cdot \mbox{\dots} \cdot F_k$, где $F_1 = S[0]$ и $F_i$ -- наибольший непустой префикс подстроки $S\substr{|F_1\cdot
\mbox{\dots} \cdot F_{i-1}|}{|S|}$, который входит в качестве подстроки в $F_1 \cdot \mbox{\dots} \cdot F_{i-1}$, а
если указанный префикс пуст, то $F_i=S[|F_1\cdot \mbox{\dots} \cdot F_{i-1}|]$. Число $k$ называется \emph{размером
факторизации}.
\end{definition}

Формулировка задачи построения ПП не накладывает строгих ограничений на дерево вывода ПП, кроме того, что оно является двоичным и
\emph{максимальным}. Последнее свойство означает, что любой внутренний узел имеет ровно двух сыновей (термин подсказан теорией кодирования
-- понятно, что двоичный префиксный код будет максимальным по включению тогда и только тогда, когда его дерево максимально в указанном
смысле). Существуют различные виды двоичных деревьев. Какой из них больше подходит для решения рассматриваемой задачи? В алгоритме
построения ПП из~\cite{SLPConstruction} используются сбалансированные деревья, а именно, AVL-деревья.

\begin{definition}
\emph{AVL-дерево} -- это двоичное дерево, у каждого внутреннего узла которого высоты сыновей отличаются не более чем на 1.
\end{definition}

Для AVL-деревьев имеется логарифмическая от числа узлов оценка высоты, см.~\cite[пп.~6.2.3]{Knuth}. Именно поэтому данный вид деревьев
используется в алгоритме Риттера. Однако алгоритм построения AVL-деревьев нетривиален и требователен к ресурсам. В качестве альтернативы в
п.\,3.4  мы рассматриваем алгоритм построения ПП на основе декартовых деревьев. Напомним соответствующие определения.

\begin{definition}
\emph{Двоичное дерево поиска} -- это двоичное дерево, в каждом узле которого хранится число, называемое \emph{ключом}. При этом для
произвольного внутреннего узла $X$ ключи всех узлов левого поддерева меньше ключа $X$, а ключи всех узлов правого поддерева больше ключа
$X$.

\emph{Куча} -- это двоичное дерево, в каждом узле которого хранится число, называемое \emph{приоритетом}. При этом для произвольного
внутреннего узла $X$ приоритеты его сыновей меньше приоритета $X$.

\emph{Декартово дерево} -- это двоичное дерево, в каждом узле которого хранится пара чисел: \emph{ключ} и \emph{приоритет}. При этом оно
является двоичным деревом поиска по ключам и кучей по приоритетам.
\end{definition}

Имеется вероятностная логарифмическая от числа узлов оценка высоты декартова дерева (\!\!\cite{Seidel&Aragon:1996}, см. п.\,3.4 ниже). При
этом алгоритм построения декартова дерева тратит существенно меньше времени на поддержание баланса узлов. Одной из основных целей данной
работы является сравнительный анализ алгоритмов построения ПП, основанных на AVL-деревьях и на декартовых деревьях.

\subsectionnew{Узкое место алгоритма Риттера.} Риттер~\cite{SLPConstruction} доказал следующий результат.

\begin{thm}
\label{thm:rytter}
Существует алгоритм, который по данному тексту $S$ длины $n$ и по его LZ-факторизации размера $k$ за время $O(k\log n)$
строит ПП, выводящую $S$ и имеющую размер $O(k\log n)$.
\end{thm}

Доказательство теоремы является конструктивным. Опишем ключевые идеи алгоритма Риттера, поскольку они важны для
дальнейшей дискуссии.

\emph{AVL-грамматиками} называются ПП, деревья вывода которых являются AVL-деревьями. Основной операцией, используемой
в алгоритме, является конкатенация AVL-грам\-ма\-тик. Следующая лемма из~\cite{SLPConstruction} оценивает сверху
трудоемкость этой операции.

\begin{lem}
\label{avl-lemma} Пусть $\slp{S}$, $\slp{S}'$ -- AVL-грамматики. Тогда, добавив не более чем $O\left(|h(\slp{S})-h(\slp{S}')|\right)$ новых
правил, за время $O\left(|h(\slp{S})-h(\slp{S}'|\right)$ можно построить AVL-грамматику $\slp{S}\cdot\slp{S}'$, которая выводит текст
$S\cdot S'$.
\end{lem}

\medskip

\header{Алгоритм Риттера} получает на вход LZ-факторизацию $F_1$, $F_2$, \dots, $F_k$ данного текста $S$ и индуктивно строит ПП $\slp{S}$,
выводящую текст $\concat{F}{1}{2}{i}$ для $i=1,2,\dots,k$.

\smallskip

\noindent\textbf{Инициализация:} Полагаем $\slp{S}$ равной грамматике, состоящей из терминального правила, выводящего $F_1=S[0]$.

\smallskip

\noindent\textbf{Основной цикл:} Предположим, что для фиксированного $i > 1$ уже построена ПП $\slp{S}$, выводящая текст
$\concat{F}{1}{2}{i}$. По определению  LZ-факторизации фактор $F_{i+1}$ является подстрокой в тексте $\concat{F}{1}{2}{i}$. Фиксируем
позиции вхождения фактора $F_{i+1}$ в текст $\concat{F}{1}{2}{i}$ и, используя алгоритм взятия подграмматики, находим правила
$\tuple{\slp{S}}{1}{2}{\ell}$ такие, что $F_{i+1} = \concat{S}{1}{2}{\ell}$. Поскольку $\slp{S}$ -- AVL-грамматика, то $\ell = O(\log
|S|)$. С помощью леммы 3.2 конкатенируем правила $\tuple{\slp{S}}{1}{2}{\ell}$ в некотором фиксированном порядке (подробнее
см.\,\cite{SLPConstruction}). Полагаем $\slp{S}:=\slp{S}\cdot(\concat{\slp{S}}{1}{2}{\ell})$.

\medskip

При работе с AVL-деревьями самой трудной задачей является поддержание баланса. Если после добавления нового правила в дерево оно перестает
быть сбалансированным, то полученное дерево приходится перебалансировать с помощью локального преобразования, называемого \emph{вращением}.
Существует два типа вращений, они схематически показаны на рис.\,\ref{avl_rotations}. Каждое вращение может порождать до трех новых узлов
(отмечены штрихами на рис.\,\ref{avl_rotations}), а также сделать до трех узлов висячими (неиспользуемыми).
\begin{figure}[th]
    \begin{center}
        \begin{picture}(100,175)(120,10)
            \AVLrotations
        \end{picture}
    \end{center}
    \caption{Вращения после вставки узла в AVL-дерево}
    \label{avl_rotations}
\end{figure}

Как видно из леммы~\ref{avl-lemma}, при конкатенации  AVL-грам\-ма\-тик существенно разной высоты появляется много новых правил, при
добавлении которых может возникнуть необходимость в большом числе вращений. Именно в этом состоит узкое место алгоритма Риттера -- когда
его основной цикл повторится достаточное число раз, высота текущей AVL-грамматики $\slp{S}$ становится большой, а при каждом последующем
выполнении основного цикла с $\slp{S}$ конкатенируется AVL-грамматика относительно небольшой высоты. Следующий простой пример
демонстрирует, что число вращений при исполнении алгоритма Риттера действительно может оказаться значительным.

\begin{example}
Пусть $S = a^{2^n}bc^{2^n}$, где $n$ -- фиксированное натуральное число.  LZ-факторизация строки $S$ имеет вид:
$$S=a \cdot a \cdot a^2 \cdot a^4 \cdot \mbox{\dots} \cdot a^{2^{n-1} - 1} \cdot b \cdot c \cdot c \cdot c^2 \cdot c^4
\cdot \mbox{\dots} \cdot c^{2^{n-1} - 1}.$$ Обозначим факторы (в том порядке, в котором они следуют в этой факторизации) через $F_1,F_2,
\dots,F_{2n+3}$, а соответствующие им ПП (деревья которых, как легко понять, будут полными двоичными деревьями, в частности, AVL-деревьями)
-- через $\slp{F}_1,\slp{F}_2,\dots,\slp{F}_{2n+3}$.

Оценим число вращений, которое потребуется при последовательной конкатенации AVL-грамматик $(\dots((\slp{F}_1 \cdot \slp{F}_2)\cdot
\slp{F}_3)\dots) \cdot \slp{F}_{2n+3}$. При последовательная конкатенации грамматик $\tuple{\slp{F}}{1}{2}{n+1}$ вращения не понадобятся,
поскольку на каждом шаге будут конкатенироваться полные двоичные деревья одинаковой высоты. Дерево грамматики $\concat{\slp{F}}{1}{2}{n+1}$
будет полным двоичным деревом высоты $n$, а конкатенация $(\concat{\slp{F}}{1}{2}{n+1})\cdot\slp{F}_{n+1}$ даст AVL-дерево высоты $n+1$.
Нетрудно проверить, что каждая из последующих конкатенаций будет нарушать баланс и приводить как минимум к одному вращению. Всего
произойдет от $n+1$ до $\Theta(n^2)$ вращений (верхняя оценка вытекает из оценки для числа новых правил, содержащейся в
лемме~\ref{avl-lemma}).
\end{example}

Заметим, что если бы алгоритм мог выбрать <<правильный>> порядок конкатенации, а именно
$$((\dots((\slp{F}_1 \cdot \slp{F}_2) \cdot\slp{F}_3) \dots ) \cdot \slp{F}_{n+1})
\cdot ((\dots((\slp{F}_{n+2} \cdot \slp{F}_{n+3}) \cdot \slp{F}_{n+4}) \dots) \cdot\slp{F}_{2n+3}),$$ то не понадобилось бы ни одного
вращения. Итак, одно из направлений оптимизации алгоритма Риттера состоит в определении <<удачного>> порядка конкатенации. Другое
направление состоит в минимизации числа запросов к грамматике. Оптимизация числа запросов к грамматике становится важной в случае, когда
размер входного текста очень велик и мы не можем хранить текущее состояние ПП в оперативной памяти. Тогда временная стоимость операции
доступа к ПП превышает стоимость вычислений в оперативной памяти. Следующий пример показывает, что число запросов к ПП можно уменьшить за
счет того, что несколько факторов могут быть обработаны вместе.

\begin{example}
Пусть $S = ba^{2^{n-1}}ba^{2^{n-2}} \cdots ba$, где $n$ -- фиксированное натуральное число. LZ-факторизация строки $S$ имеет вид:
$$S=b \cdot a \cdot a \cdot a^2 \cdot a^4 \cdot \mbox{\dots} \cdot a^{2^{n-2} - 1} \cdot ba^{2^{n-2}} \cdot ba^{2^{n-3}}
\cdot \mbox{\dots} \cdot ba.$$

Пусть $\slp{S}_1$ -- ПП, выводящая текст $S_1=ba^{2^{n-1}}$. Очевидно, что все факторы указанной LZ-факторизации, начиная с $ba^{2^{n-2}}$,
входят в строку $S_1$. Следовательно, алгоритм мог бы за одно обращение к ПП $\slp{S}_1$ вычленить из нее ПП $\slp{S}_2$, $\slp{S}_3$,
\dots, $\slp{S}_{n-1}$, выводящие тексты $ba^{2^{n-3}}$, $ba^{2^{n-4}}$, \dots, $ba$ соответственно. После этого алгоритм мог бы
конкатенировать полученные ПП в следующем порядке: $\slp{S}_1\cdot(\dots(\slp{S}_{n-3}\cdot(\slp{S}_{n-2} \cdot \slp{S}_{n-1}))\dots)$.
\end{example}

\subsectionnew{Оптимизация алгоритма Риттера}
Идея оптимизации состоит в том, чтобы обрабатывать факторы максимально возможными группами и в рамках каждой группы факторов выбирать
оптимальный порядок конкатенации. Интуитивное обоснование этой идеи таково: если уже построена <<большая>> ПП, то большинство последующих
факторов входит в текст, выводимый из нее, а значит, эти факторы могут быть обработаны вместе.

\medskip

\header{Модифицированный алгоритм Риттера} получает на вход LZ-фак\-то\-ри\-за\-цию $F_1$, $F_2$, \dots, $F_k$ данного текста $S$ и строит
ПП $\slp{S}$, выводящую $S$.

\smallskip

\noindent\textbf{Инициализация:} Полагаем $\slp{S}$ равной грамматике, состоящей из терминального правила, выводящего $F_1=S[0]$.

\smallskip

\noindent\textbf{Основной цикл:} Пусть $\slp{S}$ -- ПП, которая выводит текст $\concat{F}{1}{2}{i}$, где $0 < i < k$. Пусть
$\ell\in\{1,\dots,k-i\}$ -- наибольшее число такое, что каждый из факторов $F_{i+1}$, \dots, $F_{i+\ell}$ входит в $\concat{F}{1}{2}{i}$.
При фиксированной LZ-факторизации значение $\ell$ может быть найдено с помощью линейного поиска по факторам. ПП
$\tuple{\slp{F}}{i+1}{i+2}{i+\ell}$, которые выводят тексты $\tuple{F}{i+1}{i+2}{i+\ell}$ соответственно, вычисляются с помощью алгоритма
взятия подграмматики (аналогично алгоритму из \cite{SLPConstruction}).

Затем алгоритм вычисляет конкатенацию ПП $\tuple{\slp{F}}{i+1}{i+2}{i+\ell}$, при этом он пытается оптимизировать порядок, в котором
осуществляется конкатенация, с помощью динамического программирования. Введем функцию $\varphi(p,q)$, значения которой при $1\le
p,q\le\ell$ вычисляются по формуле:
$$\varphi(p, q) = \begin{cases}
0, &\text{если } p\ge q, \\
\min_{r = p}^{q-1}\Bigl(\varphi(p,r)+ \varphi(r+1,q)+\bigl|\log(|F_{i+p}|+{}&\\
+ \mbox{\dots} + |F_{i+r}|) - \log(|F_{i+r+1}| + \mbox{\dots} + |F_{i+q}|)\bigr|\Bigr), &\text{если } p<q.
\end{cases}$$
Значение $\varphi(p,q)$ пропорционально верхней оценке для минимального числа вращений, необходимых для конкатенации ПП
$\tuple{\slp{F}}{i+p}{i+p+1}{i+q}$, которую можно извлечь из леммы~\ref{avl-lemma} и из оценки для высоты AVL-дерева
\cite[теорема~6.2.3А]{Knuth}. Конечно, эта верхняя оценка, как правило, оказывается сильно завышенной, так что в действительности
$\varphi(p,q)$ следует рассматривать как эвристику, с помощью которой выбирается <<хорошая>> группировка факторов при конкатенации.

Заполним $\ell\times\ell$-таблицу значениями функции $\varphi(p,q)$, $1\le p,q\le\ell$; при этом для случая, когда $p<q$, вместе с
$\varphi(p,q)$ будем хранить то значение $r\in\{p,p+1,\dots,q-1\}$, при котором достигается минимум выражения
$$\varphi(p,r)+\varphi(r+1,q)+\bigl|\log(|F_{i+p}|+\mbox{\dots}+|F_{i+r}|)-\log(|F_{i+r+1}|+\mbox{\dots}+|F_{i+q}|)\bigr|.$$
Порядок заполнения таков: заполняем (нулями) все ячейки $(p,q)$ с $p\ge q$, затем заполняем ячейки с $q-p=1$, затем -- ячейки с $q-p=2$
и~т.\,д. Таким образом, к моменту подсчета очередного значения $\varphi(p,q)$ все необходимые для этого значения $\varphi(p,r)$ и
$\varphi(r+1,q)$ уже содержатся в таблице. Любое значение $\varphi(p,q)$ может быть вычислено за время $O(\ell)$ (псевдокод процедуры
представлен на рис.\,\ref{computing_fi_value}). Поэтому таблица будет заполнена за время $O(\ell^3)$ с использованием $O(\ell^2)$ памяти.
\begin{figure}[ht]
    \begin{center}
        \begin{picture}(100,105)(120,20)
            \ComputingFiValue
        \end{picture}
    \end{center}
    \caption{Псевдокод функции, вычисляющей значение $\varphi(p,q)$}
    \label{computing_fi_value}
\end{figure}

Теперь, отправляясь от значения параметра $r$, записанного в ячейке $(1,\ell)$, можно за время $O(\ell)$ восстановить тот порядок
конкатенации ПП $\tuple{\slp{F}}{i+1}{i+2}{i+\ell}$, которому соответствует значение $\varphi(1,\ell)$. Используя этот порядок, строим ПП
$\slp{F}$, которая выводит текст $\concat{F}{i+1}{i+2}{i+\ell}$, и полагаем $\slp{S}:=\slp{S}\cdot\slp{F}$.

\begin{thm}
Размер ПП, которую модифицированный алгоритм Риттера строит по данной факторизации $\tuple{F}{1}{2}{k}$ текста длины $n$, равен $O(k \log
n)$.
\end{thm}

\begin{proof1} по существу повторяет соответствующий фрагмент доказательства теоремы~\ref{thm:rytter}, но мы воспроизведем его для полноты
изложения.

Проведем индукцию по числу факторов. База индукции очевидна.

Пусть уже построена ПП $\slp{S}$, которая выводит текст $\concat{F}{1}{2}{i}$, где $0 < i < k$, и имеет размер $O(i \log
|\concat{F}{1}{2}{i}|)=O(i \log n)$. Далее, пусть $F_{i+1}$, \dots, $F_{i+\ell}$ -- все те последующие факторы, которые входят в
$\concat{F}{1}{2}{i}$. Рассмотрим подграмматики $\tuple{\slp{F}}{i+1}{i+2}{i+\ell}$ грамматики $\slp{S}$, которые выводят тексты
$\tuple{F}{i+1}{i+2}{i+\ell}$ соответственно. Высота ПП $\slp{F}_{i+j}$ не превосходит $1.4404\log|F_{i+j}|$ \cite[теорема~6.2.3А]{Knuth},
и потому по лемме~\ref{avl-lemma} число новых правил, которые придется добавить на каждом этапе вычисления ПП $\slp{F}$, которая выводит
текст $\concat{F}{i+1}{i+2}{i+\ell}$, есть $O\left(\log |F_{i+1}| + \log |F_{i+2}| + \dots + \log |F_{i+\ell}|\right)=O(\log n)$. При
балансировке грамматик каждая операция вращения порождает не более трех новых правил. Всего общее число тех правил в ПП $\slp{F}$, которые
отсутствуют в ПП $\slp{S}$, есть $O(\ell\log n)$. Аналогично, число новых правил, которые придется добавить при конкатенации $\slp{S}$ и
$\slp{F}$, есть $O(\log n)$, поэтому размер ПП $\slp{S}\cdot\slp{F}$, выводящей текст $\concat{F}{1}{2}{i+\ell}$, есть $O((i+\ell)\log n)$.
\end{proof1}

Что касается временной сложности модифицированного алгоритма Риттера, то она не может быть меньше чем сложность исходного алгоритма
из~\cite{SLPConstruction}, поскольку последний является специальным случаем предлагаемой модификации, когда размеры всех групп факторов
равны~1. C неформальной точки зрения ясно, что новый алгоритм порождает меньше вращений, но, с другой стороны, тратит дополнительное время
на вычисление оптимального порядка конкатенации. Суммарное влияние этих обстоятельств на время работы алгоритма неочевидно. В следующем
параграфе представлены результаты экспериментального сравнения обсуждаемых алгоритмов.

\subsectionnew{Построение ПП на основе декартовых деревьев}

Выше уже обсуждалось, что алгоритмы построения ПП, использующие AVL-деревья, должны тратить время на поддержание баланса в дереве. В связи
с этим возникает идея применить для представления дерева вывода другую структуру данных. В этом разделе мы опишем алгоритм, который строит
ПП, деревья вывода которых являются декартовыми деревьями.

Для декартовых деревьев имеется вероятностная логарифмическая от числа узлов оценка высоты, см. \cite[п.\,4.1]{Seidel&Aragon:1996}. А
именно, ожидаемая высота декартова дерева с $n$ узлами, приоритеты которых выбраны случайно, независимо и имеют одинаковое распределение,
есть $O(\log n)$. Более того, для любой константы $c>1$ вероятность того, что высота декартова дерева с $n$ узлами больше $2c\ln n$,
ограничена величиной $n\left(\frac{n}{e}\right)^{-c\ln(c/e)}$.

Для построения ПП по заданной факторизации исходного текста требуются две операции над деревьями вывода: операция взятия поддерева по
заданным границам и операция конкатенации двух деревьев. Для декартовых деревьев легко реализуются операции \emph{split} -- операция
разбиения декартова дерева на два поддерева по заданной границе -- и \emph{merge} -- операция слияния двух декартовых деревьев. Однако при
стандартной реализации операции \emph{merge} требуется дополнительное условие: каждый из ключей первого дерева должен быть меньше каждого
из ключей второго дерева. Следовательно, чтобы использовать декартовы деревья в алгоритме построения ПП необходимо уметь корректно
перегенерировать ключи для деревьев, полученных после применения операции \emph{split}. Эта задача возникает в основном цикле алгоритма,
когда алгоритм уже построил некоторое дерево $T$; для обработки следующего фактора алгоритм должен вырезать отвечающее этому фактору
поддерево $T'$ из $T$ и затем должен выполнить операцию \emph{merge} для $T$ и $T'$. Перед слиянием необходимо полностью перегенерировать
ключи для дерева $T'$. Оказывается, что для упрощения этой процедуры выгодно отказаться от явного хранения ключей. Поясним, почему это
возможно\footnote{К сожалению, изящная идея декартова дерева без явного хранения ключей, насколько нам известно, пока не освещалась в
академической литературе. Достаточно полно эта идея представлена в интернет-публикации~\cite{Pol10}. Достоверно
известно, что декартовы деревья с неявными ключами возникли в 2002 г.\ в олимпиадном программировании, а авторами идеи
были Н.\,В.\,Дуров (член студенческой сборной СПбГУ) и А.\,С.\,Лопатин (?)}.

Пусть $T$ -- произвольное декартово дерево и пусть информация о его ключах была утеряна. Тогда по структуре дерева всегда можно
восстановить отношение линейного порядка на ключах. Для этого обойдем дерево рекурсивно в следующем порядке: левое поддерево, корень,
правое поддерево. Тогда порядковый номер ключа данного узла среди всех ключей узлов дерева на единицу больше числа узлов в той части
дерева, которое мы обошли до того, как попали в данный узел. Таким образом, можно отказаться от явного хранения значений ключей.

\begin{definition}
\emph{Декартовым деревом с неявным ключом} называется декартово дерево, в узлах которого не хранятся значения ключей.
\end{definition}

В дальнейшем под значением ключа узла декартова дерева $T$ с неявным ключом мы будем подразумевать порядковый номер ключа этого узла в
линейном упорядочении ключей для $T$. Поддерево с корнем в узле $T_i$ обозначим через $\overline{T_i}$, а число узлов в этом поддереве --
через $\cnt(T_i)$. Запись вида $T_i = (T_\ell, T_r)$ будем использовать для обозначения того факта, что узлы $T_\ell$ и $T_r$ суть
соответственно левый и правый сыновья узла $T_i$; при этом в правой части такой записи узлы $T_\ell$ и/или $T_r$ могут быть пустыми
(например, если $T_i$ -- лист, то и $T_\ell$, и $T_r$ пусты). Аналогично, запись вида $T=(L,R)$ означает, что $L$ и $R$ суть соответственно
левое и правое поддеревья дерева $T$; при этом возможно, что $L$ и/или $R$ пусты.

Опишем, как реализуются операции \emph{split} и \emph{merge} для декартовых деревьев с неявным ключом.

\subparagraph*{Операция \emph{split}.} Эта операция получает на вход декартово дерево $T$ с неявным ключом и целое положительное число
$k\le|T|+1$ и возвращает такую пару декартовых деревьев $L$ и $R$ с неявным ключом, что $L$ содержит все узлы дерева $T$ со значениями
ключа, меньшими $k$, а $R$ содержит все остальные узлы дерева $T$. Условимся, что на входе (пустое дерево,1) операция \emph{split}
возвращает пару пустых деревьев.

Алгоритм, реализующий \emph{split}, работает рекурсивно, начиная с корня $T_0$ дерева $T$. Пусть $T_0 = (T_\ell,T_r)$. Возможны два случая.
\begin{itemize}
  \item[(S1)] Если $k\le\cnt(T_\ell) + 1$, то корень $T_0$ должен попасть в дерево $R$ и нужно разрезать поддерево $\overline{T}_\ell$. Если \emph{split}
  на входе $(\overline{T}_\ell,k)$ возвращает пару деревьев $L'$ и $R'$, то $L = L'$, a $R =(R',\overline{T}_r)$.
  \item[(S2)] Если $k >\cnt(T_\ell) + 1$, то корень $T_0$ должен попасть в дерево $L$ и нужно разрезать поддерево $T_r$.
  Если \emph{split} на входе  $(\overline{T}_r,k -\cnt(T_\ell)-1)$ возвращает пару деревьев $L'$ и $R'$,
  то $L = (\overline{T}_\ell, L')$, а $R = R'$.
\end{itemize}
Поскольку на каждом шаге алгоритм либо завершает работу, либо выполняет рекурсивный вызов с узлом меньшей высоты, ожидаемое время работы
алгоритма есть $O(\log|T|)$, если хранить в каждом узле $T_i$ число $\cnt(T_i)$.

\medskip

Мы будем использовать \emph{split} при построении ПП, а по определению ПП как грамматики в нормальной форме Хомского дерево вывода ПП
должно быть максимальным. Поэтому нужно немного модифицировать операцию, чтобы гарантировать, что возвращаемые ей декартовы деревья
максимальны. Сделать это очень просто -- достаточно удалить из каждого из деревьев, возвращаемых \emph{split}, все узлы, имеющие ровно
одного сына. Более формально, если узел $T_j$ -- отец единственного сына $T_k$, то узел $T_j$ удаляется из дерева. В новом дереве узел
$T_k$ объявляется корнем, если $T_j$ был корнем, и сыном того узла, сыном которого был $T_j$, если $T_j$ не был корнем. Приоритеты узлов
при этом не изменяются.

Ясно, что если исходное дерево $T$ максимально, то на каждом шаге алгоритма, реализующего \emph{split}, в каждом из деревьев $L$ и $R$
возникает максимум один узел с единственным сыном. Поэтому ожидаемое время выполнения описанной только что процедуры <<максимизации>>
деревьев $L$ и $R$ есть $O(\log|T|)$. В действительности, при программной реализации эта процедура не требует отдельного прохода, так как
она может быть просто встроена в алгоритм. В дальнейшем, говоря об операции \emph{split}, мы всегда будем иметь в виду ее модификацию,
возвращающую максимальные деревья.

\subparagraph*{Операция \emph{merge}.} Эта операция получает на вход пару декартовых деревьев $T'$ и $T''$ с неявным ключом и возвращает
декартово дерево $T$ с неявным ключом, содержащее все узлы из $T'$ и $T''$. Условимся, что если дерево $T'$ пусто, то операция возвращает
дерево $T''$, и наоборот, если  $T''$ пусто, то возвращается $T'$.

Алгоритм, реализующий \emph{merge}, работает рекурсивно, начиная с корней $T'_0$ и $T''_0$ деревьев $T'$ и $T''$. Пусть $T'_0 = (T'_{\ell},
T'_{r})$ и $T''_0 = (T''_{k}, T''_{q})$. Поскольку приоритеты узлов выбираются случайно и независимо, можно предполагать, что все они
различны. Тогда возможны два случая.
\begin{itemize}
  \item[(M1)] Если приоритет узла $T'_0$ больше приоритета узла $T''_0$, то дерево $T$ имеет в качестве корня узел $T'_0$. Левым поддеревом
  для корня служит дерево $\overline{T}'_{\ell}$, а правым -- то дерево, которое  \emph{merge} возвращает на входе
  $(\overline{T}'_{r},T'')$.
  \item[(M2)] Если приоритет узла $T'_0$ меньше приоритета узла $T''_0$, то дерево $T$ имеет в качестве корня узел $T''_0$. Правым поддеревом
  для него служит дерево $\overline{T}''_{q}$, а левым -- то дерево, которое  \emph{merge} возвращает на входе
  $(T',\overline{T}''_k)$.
\end{itemize}
Так как на каждом шаге рекурсии алгоритм спускается либо внутри первого дерева, либо внутри второго дерева, математическое ожидание времени
работы алгоритма есть $O(\log|T'| + \log |T''|)$.

\medskip

Так же, как и в случае \emph{split}, придется немного модифицировать операцию \emph{merge}, чтобы приспособить ее к построению ПП. Здесь
возникают две проблемы. Во-первых, мы должны гарантировать максимальность возвращаемого декартова дерева. Во-вторых, поскольку операция
\emph{merge} будет использоваться для конкатенации ПП, необходимо, чтобы массив листьев дерева $T$ получался приписыванием к массиву
листьев дерева $T'$ массива листьев дерева $T''$. Оказывается, что обе эти проблемы одновременно решаются следующей простой модификацией
стандартного алгоритма.

Пусть $T'_i$ -- крайний правый лист дерева $T'$, а $T''_j$ -- крайний левый лист дерева $T''$ (заметим, что в декартовых деревьях с неявным
ключом такие <<крайние>> листья определяются однозначно), и пусть $y'$ и соответственно $y''$ -- приоритеты этих листьев. Положим
$y_*=\min(y',y'')$, $y^*=\max(y',y'')$. Переопределим приоритеты узлов $T'_i$ и $T''_j$, полагая их равными $y_*$. Тогда алгоритм, выполняя
правила (M1) и (M2), необходимо придет к конфигурации, в которой роль текущих корней $T'_0$ и $T''_0$ играют листья $T'_i$ и $T''_j$
соответственно. На этом шаге введем новый узел $U=(T'_i,T''_j)$ c приоритетом $y^*$ и завершим построение дерева $T$ добавлением
трехэлементного поддерева
\begin{picture}(54,18)(-10,2)\put(1,-1){\line(1,1){14}}\put(15,13){\line(1,-1){14}}\put(17,14){$U$}\put(-10,0){$T'_i$}\put(31,0){$T''_j$}\end{picture}
вместо того двухэлементного поддерева
(\begin{picture}(36,20)(-10,2)\put(1,-1){\line(1,1){14}}\put(17,14){$T''_j$}\put(-10,0){$T'_i$}\end{picture} или
\begin{picture}(32,20)(-10,2)\put(-10,13){\line(1,-1){14}}\put(-7,14){$T'_i$}\put(7,0){$T''_j$}\end{picture}), которое добавил бы
стандартный алгоритм. Ясно, что модифицированный таким образом алгоритм выполняется за то же ожидаемое время $O(\log|T'| + \log |T''|)$, и
нетрудно проверить, что возвращаемое им дерево $T$ максимально, если максимальны деревья $T'$ и $T''$. Более того, $T$ является
конкатенацией $T'$ и $T''$ в смысле ПП, см. \S2. В дальнейшем, говоря об операции \emph{merge}, мы всегда будем иметь в виду ее описанную
модификацию.

Теперь приведем алгоритм построения <<декартовой>> ПП, т.\,е.\ ПП, дерево вывода которой является декартовым деревом с неявным ключом.

\medskip

\header{Алгоритм построения декартовой ПП} получает на вход LZ-фак\-то\-ри\-за\-цию $F_1$, $F_2$, \dots, $F_k$ данного текста $S$ и
индуктивно строит декартову ПП $\slp{S}$, выводящую текст $\concat{F}{1}{2}{i}$ для $i=1,2,\dots,k$.

\smallskip

\noindent\textbf{Инициализация:} Полагаем $\slp{S}$ равной грамматике, состоящей из терминального правила, выводящего $F_1=S[0]$.

\smallskip

\noindent\textbf{Основной цикл:} Предположим, что для фиксированного $i>1$ уже построена декартова ПП $\slp{S}$, выводящая текст
$\concat{F}{1}{2}{i}$. По определению  LZ-факторизации фактор $F_{i+1}$ является подстрокой в тексте $S=\concat{F}{1}{2}{i}$. Фиксируем
такие позиции $\ell$ и $r$ в тексте $S$, что $F_{i+1}=S\substr{\ell}{r}$. Пусть $\ell^*$ и $r^*$ -- значения ключей для листьев $S[\ell]$ и
соответственно $S[r]$ дерева $\slp{S}$. Заметим, что значения $\ell^*$ и $r^*$ легко вычисляются по $\ell$ и соответственно $r$, если
хранить в каждом узле $\slp{S}_i$ число $\cnt(\slp{S}_i)$.

Применим операцию \emph{split} к входу $(\slp{S}, \ell^*)$ и пусть $R$ -- правое из деревьев, возвращаемых этой операцией. Теперь применим
\emph{split} к входу $(R,r^*-\ell^*)$. Левое из возвращаемых деревьев даст декартову ПП $\slp{F}$, которая выводит текст $F_{i+1}$.
Применение операции \emph{merge} к $\slp{S}$ и $\slp{F}$ дает декартову ПП, которая выводит текст $\concat{F}{1}{2}{i+1}$.

\begin{thm}
\label{thm:cartesian} Математическое ожидание времени работы описанного алгоритма на тексте $S$ длины $n$ и его LZ-факторизации размера $k$
равно $O(k\log n)$. Математическое ожидание размера ПП, возвращаемой алгоритмом, также равно $O(k\log n)$.
\end{thm}

\begin{proof}
На каждом шаге алгоритма запускается не более двух раз операция \emph{split} и не более одного раза операция \emph{merge}. Из описания этих
операций следует, что математическое ожидание времени работы каждого шага алгоритма есть $O(\log n)$, а так как алгоритм делает ровно $k$
шагов, математическое ожидание всего времени работы алгоритма есть $O(k \log n)$.

Для того, чтобы оценить размер декартовой ПП, возвращаемой алгоритмом, заметим, что на каждом шаге каждой из операций \emph{split} и
\emph{merge} возникает не более одного нового нетерминального правила. Ожидаемое число шагов каждой операции есть $O(\log n)$, а в сумме
обе операции выполняются $3k$ раз. Поэтому математическое ожидание размера ПП, возвращаемой алгоритмом, есть $O(k \log n)$.
\end{proof}

\sectionnew{Экспериментальные результаты}

\subsectionnew{Условия экспериментов.}
Очевидно, что природа исходного текста влияет как на скорость, так и на степень сжатия. В наших экспериментах использовались тексты
следующих трех типов:
\begin{itemize}
  \item строки Фибоначчи;
  \item случайные строки над четырехбуквенным алфавитом;
  \item последовательности ДНК, взятые из открытого банка ДНК Японии \newline (\url{http://www.ddbj.nig.ac.jp/}).
\end{itemize}
Выбор этих типов текстов был обусловлен такими соображениями. Строки Фибоначчи -- это в определенном смысле наилучшие входные данные для
задачи построения ПП, и эксперименты над ними позволяют оценить потенциал ПП как модели сжатого представления <<сверху>>. Напротив,
случайные строки сжимаются плохо и, следовательно, являются наихудшим входом для задачи построения ПП. Поэтому эксперименты со случайными
строками оценивают возможности ПП <<снизу>>. Наконец, последовательности ДНК -- это практически важный класс хорошо сжимаемых строк.

Мы сравниваем алгоритмы построения ПП из \S3 с классическими алгоритмами сжатия из семейства Лемпеля-Зива. Наш тестовый набор алгоритмов
содержит два варианта алгоритма Лемпеля-Зива \cite{LZ77}: алгоритм с малым (32KБ) размером окна сжатия и алгоритм с бесконечным окном
сжатия, а также алгоритм Лемпеля-Зива-Велча \cite{LZW}. Исходный код проекта доступен по адресу
\url{http://code.google.com/p/overclocking/}. Все алгоритмы запускались в одинаковых условиях на компьютере со
следующими параметрами: процессор Intel Core i7-2600 с тактовой частотой 3.4GHz, 8ГБ оперативной памяти, операционная система Windows 7 x64. 

\subsectionnew{Результаты экспериментов.}
Как и ожидалось, все алгоритмы построения ПП работают бесконечно быстро на строках Фибоначчи и строят очень компактные ПП. Например, 35-е
слово Фибоначчи длиной порядка 40МБ обрабатывается в течение 1мс, а на выходе получается ПП, имеющая около 100 правил.

Основные результаты наших экспериментов для случайных строк и последовательностей ДНК представлены графиками на
рис.\,\ref{dna_rotations}--\ref{random_compression_ratio}. На этих графиках приняты следующие обозначения для данных, относящихся к различным алгоритмам:
    \begin{center}
        \begin{picture}(100,70)(100,10)
            \algorithmNotations
        \end{picture}
    \end{center}
По оси абсцисс на всех графиках откладывается длина сжимаемого текста.

Основными параметрами алгоритмов, которыми мы интересовались, были время работы и степень сжатия. Степень сжатия текста определялась как
отношение размера сжатого представления текста к длине текста, выраженное в процентах. Для алгоритмов построения ПП с помощью AVL-деревьев
подсчитывалось также число произведенных операций вращения.

\begin{figure}[th]
    \begin{center}
        \begin{picture}(100,250)(95,10)
            \DNARotations
        \end{picture}
        \begin{picture}(100,170)(0,10)
            \RandomRotations
        \end{picture}
    \end{center}
    \caption{Число вращений AVL-дерева (по оси ординат) при построении ПП для последовательностей ДНК (слева) и для случайных строк (справа)}
    \label{dna_rotations}
\end{figure}

Риc.\,\ref{dna_rotations} демонстрирует, как сказывается описанная в п.\,3.3 модификация алгоритма Риттера на числе вращений. Видно, что
уже для текстов длиной около 10MБ модифицированный алгоритм использует на порядок меньше вращений. Это свидетельствует об эффективности
предложенной эвристики. Обращает на себя внимание то обстоятельство, что для обоих алгоритмов число вращений довольно регулярно зависит от
длины входного текста и в то же время мало зависит от его природы. Мы не располагаем никаким теоретическим объяснением для этих наблюдений.

\begin{figure}[p]
    \begin{center}
        \begin{picture}(100,230)(150,10)
            \DNATimeStats
        \end{picture}
    \end{center}
    \caption{Время работы алгоритмов построения ПП на последовательностях ДНК при хранении ПП в памяти (слева) и в файле (справа)}
    \label{dna_time_stats}
    \begin{center}
        \begin{picture}(100,230)(160,10)
            \RandomTimeStats
        \end{picture}
    \end{center}
    \caption{Время работы алгоритмов построения ПП на случайных строках при хранении ПП в памяти (слева) и в файле (справа)}
    \label{random_time_stats}
\end{figure}

Как уже обсуждалось в п.\,3.3, выигрыш в числе вращений еще не гарантирует выигрыш в скорости построения ПП, поскольку модифицированный
алгоритм тратит дополнительное время на вычисление оптимального порядка конкатенации. Для сравнения скорости алгоритмов из \S3 были
проведены тесты двух типов. В тестах первого типа алгоритмы хранили строящееся дерево в оперативной памяти, а в тестах второго типа -- во
внешнем файле (т.\,е.\ каждое обращение к дереву было обращением к файловой системе). На рис.\,\ref{dna_time_stats}
и~\ref{random_time_stats} представлены результаты тестов обоих типов соответственно на последовательностях ДНК и на случайных строках.
Видно, что при хранении ПП в оперативной памяти время работы модифицированного алгоритма, описанного в п.\,3.3, в 
среднем в несколько раз меньше чем время работы исходного алгоритма Риттера (в два раза меньше на произвольных строках,
в три раза меньше на последовательностях ДНК). Если же дерево хранится в файловой системе, то модифицированный алгоритм
работает в среднем в пять раз быстрее на последовательностях ДНК и в три раза быстрее на произвольных строках. Алгоритм,
строящий декартовы деревья, превосходит по быстродействию исходный алгоритм Риттера, однако немного уступает его
модифицированной версии. Причина здесь, по-видимому, связана с тем, что высота декартова дерева, возвращаемого
алгоритмом, существенно больше высоты соответствующих AVL-деревьев. (Средняя высота AVL-дерева в наших экспериментах
равна 21.8, а средняя высота декартова дерева составляет 47.8.) Из-за большей высоты дерева алгоритму построения
декартовых ПП приходится обрабатывать намного больше правил, что сводит на нет весь выигрыш, который возникает за счет
простоты поддержания баланса в декартовых деревьях.

На рис.\,\ref{random_compression_ratio} степень сжатия, достигаемая алгоритмами построения ПП, сравнивается со степенью сжатия алгоритмов
из семейства Лемпеля-Зива. Видно, что алгоритмы, строящие AVL-деревья, обеспечивают практически одинаковую степень сжатия, которая хуже
степени сжатия алгоритмов Лемпеля-Зива(-Велча) примерно в два раза. Интересно, что отношение степени сжатия алгоритмов, строящих
AVL-деревья, к степени сжатия алгоритмов Лемпеля-Зива(-Велча) по существу не зависит ни от типа сжимаемого текста, ни от его длины. У
алгоритма, строящего декартовы ПП, степень сжатия заметно хуже, чем у алгоритмов, строящих AVL-деревья. И здесь можно отметить, что
отношение между степенями сжатия мало изменяется при изменении типа и/или размера входных данных.

\begin{figure}[p]
    \begin{center}
        \begin{picture}(100,250)(100,-50)
            \resizebox{10cm}{11cm}{
                \CompressionRatioOnDNAs
            }
        \end{picture}
    \end{center}
    \begin{center}
        \begin{picture}(100,250)(100,15)
            \resizebox{10cm}{11cm}{
                \CompressionRatioOnRandoms
            }
        \end{picture}
    \end{center}
    \caption{Степень сжатия на последовательностях ДНК (сверху) и на случайных строках (снизу)}
    \label{random_compression_ratio}
\end{figure}

\sectionnew{Выводы и дальнейшие перспективы}

Наши эксперименты демонстрируют, что предложенная модификация алгоритма Риттера не уступает исходной
версии этого алгоритма по достигаемой степени сжатия и заметно превосходит ее по скорости построения ПП. Поскольку с
возрастанием длины входного текста использование файловой системы становится неизбежным, можно заключить, что
модифицированный алгоритм также более устойчив к росту входных данных.

Другой алгоритм, представленный в работе, использует для построения ПП декартовы деревья. Он близок по скорости работы
к алгоритмам, основанным на AVL-деревьях, по скорости, но ощутимо уступает им по степени сжатия и по высоте полученного
сжатого представления. Последнее обстоятельство существенно с точки зрения влияния на быстродействия алгоритмов,
работающих непосредственно со сжатыми представлениями данных. Таким образом, цель алгоритмически ускорить построение ПП
с помощью <<прогрессивной>> структуры данных нельзя считать достигнутой, более того, такая цель сейчас представляется
нам труднодостижимой. По-видимому, большой отдачи можно ожидать от поиска новых эвристик, позволяющих строить более
компактные ПП, основанные на AVL-деревьях.

Все проанализированные алгоритмы сжатия на основе ПП уступают алгоритмам из семейства Лемпеля-Зива по степени сжатия и по времени
сжатия. Теоретическое преимущество алгоритмов на основе ПП состоит в том, что они обеспечивают хорошо структурированное сжатое
представление, для которого имеются алгоритмы, позволяющие решать некоторые важные задачи (типа поиска подстроки) без разархивирования.
Однако вопрос о том, на каких объемах входных данных алгоритмы, работающие с ПП, смогут обогнать классические строковые алгоритмы, остается
открытым и, на наш взгляд, является одним из основных направлений для дальнейших исследований в рассматриваемой области.

\bigskip

Авторы выражают свою благодарность профессору М.В. Волкову за его критические замечания и всестороннюю поддержку
их деятельности. Авторы благодарны анонимному рецензенту за его замечания и предложения по улучшению исходного текста
работы.

\small

\begin{thebibliography}{99}
\bibitem{Knuth}
\textsl{Д.\,Кнут}, Искусство программирования, том 3. Сортировка и поиск, 2-е изд. М.: «Вильямс», 2007.

\bibitem{Pol10}
\textsl{А.\,Полозов}, Декартово дерево: Часть 3. Декартово дерево по неявному ключу, Электронный ресурс,
\url{http://habrahabr.ru/blogs/algorithm/102364/}.

\bibitem{RunLengthEncoding}
\textsl{A.\,Apostolico, G.\,M.\,Landau, S.\,Skiena}, Matching for Run-Length Encoded Strings, J. Complexity, 15 (1999),
4--16.

\bibitem{BKh11}
\textsl{I.\,Burmistrov, L.\,Khvorost}, Straight-line programs: a practical test, Proc. Int. Conf. Data Compression, Commun., Process., CCP
(2011), 76--81.

\bibitem{SmallestCFG}
\textsl{M.\,Charikar, E.\,Lehman,  D.\,Liu, R.\,Panigrahy, M.\,Prabhakaran, A.\,Sahai, A.\,Shelat}, The smallest grammar problem, IEEE
Trans. Information Theory, 51 (2005), 2554--2576.

\bibitem{collages}
\textsl{T.\,Kida, T.\,Matsumoto, Y.\,Shibata, M.\,Takeda, A.\,Shinohara, S.\,Arikawa}, Collage system: a unifying
framework for compressed pattern matching, Theor. Comput. Sci., 298 (2003), 253--272.

\bibitem{PM_and_HD}
\textsl{Y.\,Lifshits}, Processing compressed texts: A tractability border, Lect. Notes Comput. Sci., 4580 (2007), 228--240.

\bibitem{LCSubstring}
\textsl{W.\,Matsubara, S.\,Inenaga, A.\,Ishino, A.\,Shinohara, T.\,Nakamura, K.\,Hashimoto}, Computing longest common
substring and all palindromes from compressed strings, Lect. Notes Comput. Sci., 4910 (2008), 364--375.

\bibitem{SLPConstruction}
\textsl{W.\,Rytter}, Application of {L}empel-{Z}iv factorization to the approximation of grammar-based compression,
Theor. Comput. Sci., 302 (2003), 211--222.

\bibitem{Seidel&Aragon:1996}
\textsl{R.\,Seidel, C.\,Aragon}, Randomized search trees, Algorithmica 16 (1996), 464--497.

\bibitem{antidictionaries}
\textsl{Y.\,Shibata, M.\,Takeda, A.\,Shinohara, S.\,Arikawa}, Pattern matching in text compressed by using
antidictionaries, Lect. Notes Comput. Sci., 1645 (1999), 37--49.

\bibitem{LCS_P}
\textsl{A.\,Tiskin}, Faster subsequence recognition in compressed strings, Journal of Mathematical Sciences, 158 (2009),
759—769.

\bibitem{LZW}
\textsl{T.\,Welch}, A technique for high-performance data compression, IEEE Computer, 17 (1984), 8--19.

\bibitem{LZ77}
\textsl{J.\,Ziv, A.\,Lempel}, A universal algorithm for sequential data compression, IEEE Trans. Information Theory, 23 (1977), 337--343.

\bibitem{LZ78}
\textsl{J.\,Ziv, A.\,Lempel}, Compression of individual sequences via variable-rate coding, IEEE Trans. Information Theory, 24 (1978),
530--536.

\end{thebibliography}

\end{document}
