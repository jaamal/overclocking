\documentclass[cp1251,usehyperref,14pt]{G7-32}

\usepackage[T2A]{fontenc}
\usepackage[russian]{babel} 
\usepackage {listings}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{indentfirst}
\usepackage{float}
\usepackage{morefloats}
\usepackage{longtable}
\usepackage{setspace}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{textcomp}
\usepackage{setspace}

\onehalfspacing

\usepackage{indentfirst}     
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}


\usepackage{pgf}
\usepackage{tikz}
\usepackage{document}
\usepackage{pictures}

\DeclareCaptionLabelSeparator{dot}{. }
\captionsetup{justification=centering,labelsep=dot}  

\geometry{left=2cm}
\geometry{right=1.5cm}
\geometry{top=2cm}
\geometry{bottom=2cm}

\TableInChapter
\PicInChapter

\gdef\theequation{\arabic{chapter}.\arabic{equation}}

% Определяем заголовки для титульной страницы

\NirOrgLongName{\textbf{Федеральное государственное автономное образовательное учреждение \\
высшего профессионального образования \\
<<Уральский федеральный университет \\
имени первого Президента России Б.Н.Ельцина>>\\
\ \\
Уральский региональный центр \\
образования и разработок
}}

\NirBoss{Директор Уральского регионального центра образования и разработок кандидат физико-математических наук,
доцент}{Асанов М.О.}

\NirManager{младший науный сотрудник НИИ ФПМ}{Хворост А.А.}

\NirYear{2011}

\NirTown{г. Екатеринбург,}

\NirUdk{УДК }
\NirGosNo{Код ГРНТИ}
\NirStage{вид отчета:}{}{заключительный}

\bibliographystyle{unsrt}

%%%%%%%<------------- НАЧАЛО ДОКУМЕНТА

\begin{document}

\usefont{T2A}{ftm}{m}{} %%% Использование шрифтов Т2 для возможности скопировать текст из PDF-файлов.

\frontmatter %%% <-- это выключает нумерацию ВСЕГО; здесь начинаются ненумерованные главы типа Исполнители, Обозначения и прочее

\NirTitle{по проекту \No \  ООК 6 \quad <<Алгоритмические свойства сжатых текстов>> \\ 
в рамках Открытого окружного конкурса студенческих инициативных научных \\ 
исследований в области информатики и информационных технологий}

\Executors %% Список исполнителей здесь

\begin{longtable}{p{0.55\linewidth}p{0.2\linewidth}p{0.3\linewidth}}

Научный руководитель, 	& \rule{1\linewidth}{0.1pt} & \emph{Хворост А.А.}\\

\emph{младший науный сотрудник НИИ ФПМ}	& \qquad \footnotesize{подпись, дата} &\footnotesize{(раздел(ы)\underline{~~~~~~~~~})} \\

& & \\

Исполнители  &		&	\\

\emph{студентка КН-402, института} & \rule{1\linewidth}{0.1pt}& \emph{Козлова А.В.} \\

\emph{математики и компьютерных наук УрФУ}& \qquad \footnotesize{подпись, дата} &
\footnotesize{(раздел(ы)\underline{~~~~~~~~~})}\\

\\

\emph{студент КН-402, института } & \rule{1\linewidth}{0.1pt}& \emph{Курпилянский Е.Б.}\\

\emph{математики и компьютерных наук УрФУ}& \qquad \footnotesize{подпись, дата}	&
\footnotesize{(раздел(ы)\underline{~~~~~~~~~})}\\

& & \\

Нормоконтролер  &  \rule{1\linewidth}{0.1pt} &  \\

& \qquad \footnotesize{подпись, дата}	&  \\

\end{longtable}

\Referat 

\textsc{сжатие данных без потерь, контекстно-свободные грамматики, прямолинейные программы, avl-деревья, декартовы
деревья, семейство алгоритмов Лемпеля-Зива, lz-факторизация}

Объектом исследования являются алгоритмы сжатия данных без потерь.

Цель работы --- разработка эффективных алгоритмов сжатия данных с помощью контекстно-свободных грамматик.

В рамках научно-исследовательской работы были спроектированы два алгоритма построения контекстно-свободных грамматик.
Первый алгоритм строит контекстно-свободную грамматику на основе AVL-деревьев. Второй алгоритм строит 
контекстно-свободную грамматику на основе декартовых деревьев. Получена теоретическая оценка сложности обоих алгоритмов.
Поскольку оба алгоритма принадлежат одному классу сложности, то целесообразно сравнить их эффективность на практике.

Полученные алгоритмы впервые были реализованы на языке программирования Java SE. 

В процессе работы проводились экспериментальные исследования эффективности полученных алгоритмов сжатия на текстах
различной природы. В качестве входных данных использовались тексты трех видов: строки Фибоначчи, последовательности
ДНК, произвольные строки над конечным алфавитом.

Основные конструктивные показатели алгоритма на основе AVL-деревьев: удовлетворительная скорость построения сжатого
представления, высокая степень сжатия данных.

Основные конструктивные показатели алгоритма на основе декартовых деревьев: высокая скорость построения
сжатого представления, удовлетворительная степень сжатия данных.

Степень внедрения --- сравнительный анализ полученных алгоритмов сжатия и классических алгоритмов сжатия был выложен в
открытый доступ по адресу \url{http://overclocking.usu.edu.ru}.

Оба алгоритма могут применяться для сжатия текстовых данных.

\tableofcontents

\NormRefs 

\Defines

В настоящем отчете о НИР применяют следующие термины с соответствующими определениями.

\emph{Строкой} называется последовательность символов. В работе рассматриваются строки над конечным алфавитом $\Sigma$.
\textit{Длина} строки $S$ равна числу символов в $S$ и обозначается $|S|$. \textit{Конкатенация} двух строк $S_1$ и
$S_2$  обозначается $S_1 \cdot S_2$. \textit{Позицией} в строке $S$ называется место между соседними символами. Мы
нумеруем позиции произвольной строки $S$ слева направо, начиная с 1 и заканчивая $|S| - 1$. Дополнительно удобно ввести
позиции 0, которая предшествует строке $S$, и $|S|$, которая следует за $S$. Для строки $S$ и числа $0 \leq i < |S|$
обозначим через $S[i]$ символ, расположенный между позициями $i$ и $i + 1$. Например, $S[0]$ является первым символом
строки $S$. Обозначим через $S\substr{\ell}{r}, (0 \leq \ell < r \leq |S|)$ подстроку $S$, которая начинается с позиции
$\ell$ и заканчивается в позиции $r$. Таким образом $S\substr{\ell}{r} = S[\ell] \cdot S[\ell + 1] \cdot \mbox{\dots} \cdot S[r-1]$.

\textit{Прямолинейная программа} $\slp{S}$ -- последовательность правил вывода следующего вида: $$\slp{S}_1 = expr_1,
\slp{S}_2 = expr_2, \dots, \slp{S}_n = expr_n,$$ где $\slp{S}_i$ называются \textit{правилами}, а $expr_i$ называются
\textit{выражениями}. Выражения бывают двух видов: 

\begin{itemize}
\item $expr_i$ является символом $\Sigma$ (такие правила называются \emph{терминальными}),
\item $expr_i = \slp{S}_\ell \cdot \slp{S}_r \ (\ell, r < i)$ (такие правила называются \emph{нетерминальными}).
\end{itemize}

Формально, ПП это контекстно-свободная грамматика в нормальной форме Хомского, порождающая в точности одну строку над
$\Sigma^+$. 

\textbf{Пример.} Рассмотрим ПП $\slp{F}_7$, которая порождает 7-е слово Фибоначчи $F_7$=
\emph{a~b~a~a~b~a~b~a~a~b~a~a~b}:
\begin{center}
$\slp{F}_1 = a,\ \slp{F}_2 = b,\ \slp{F}_3 = \slp{F}_1\cdot \slp{F}_2,\ \slp{F}_4 = \slp{F}_3\cdot \slp{F}_1,$

$\slp{F}_5 = \slp{F}_4\cdot \slp{F}_3,\ \slp{F}_6 = \slp{F}_5\cdot \slp{F}_4,\ \slp{F}_7 = \slp{F}_6\cdot
\slp{F}_5$;
\end{center}

На рисунке \ref{FibonacciWordSLP} представлено дерево вывода ПП $\slp{F}_7$.

Строку $S$, выводимую из ПП $\slp{S}$, мы будем называть \emph{текстом}. Для ПП $\slp{S}$, выводящей текст
$S$, определим \emph{дерево вывода} $S$ через дерево вывода грамматики $\slp{S}$. В дереве вывода мы отождествляем
терминальные узлы с их родителями, поэтому оно является бинарным. В работе приняты следующие договоренности: все ПП
обозначаются с помощью заглавных фигурных букв, например, $\slp{S}$. Каждое правило ПП $\slp{S}$ (и каждый внутренний
узел ее дерева вывода) обозначается аналогичной буквой с порядковым номером, например, $\slp{S}_i$. \emph{Размер} ПП
$\slp{S}$ полагается равным числу правил и обозначается через $|\slp{S}|$. \emph{Конкатенацией} ПП $\slp{S}_1$ и
$\slp{S}_2$ называется прямолинейная программа, которая выводит текст $S_1 \cdot S_2$, и обозначается $\slp{S}_1 \cdot
\slp{S}_2$. Для удобства прямолинейная программа отождествляется с ее деревом вывода.

\begin{figure}
	\caption{Дерево вывода ПП, которая выводит 7-е слово Фибоначчи.}\label{FibonacciWordSLP}
	\begin{center}
		\FibonacciWordSLP
	\end{center}
\end{figure}

\emph{Высота} узла бинарного дерева определяется рекурсивно. Высота терминального узла (листа) полагается равной 0.
Высота нетерминального узла полагается равной 1 + максимальная из высот его детей. Обозначим через $h(\slp{S}_i)$ высоту
правила $\slp{S}_i$. \emph{Глубиной k-го узла} будем называть количество узлов, которые являются предками этого узла и
обозначать через $depth_k$. \emph{AVL-дерево} -- бинарное дерево такое, что высоты детей каждого его внутреннего узла
отличаются не более чем на 1.  \emph{Декартово дерево} -- это бинарное дерево такое, что в каждом узле хранится пара
ключей $x$ и $y$. При этом оно является бинарным деревом поиска по ключам $x$ и кучей по ключам $y$. \emph{Декартовым
деревом с неявным ключом} называется декартово дерево, в котором не хранится информация о ключах $x$.

LZ-факторизацией текста $S$ называется последовательность строк \newline $F_1, F_2, \dots, F_k$ такая, что $S = F_1 \cdot
F_2 \cdot \mbox{\dots} \cdot F_k$, $F_1 = S[0]$, $F_i$ -- наибольший префикс  $S\substr{|F_1\cdot \mbox{\dots} \cdot
F_{i-1}|}{|S|}$, который входит в качестве подстроки в $F_1 \cdot \mbox{\dots} \cdot F_{i-1}$ или $S[|F_1\cdot
\mbox{\dots} \cdot F_{i-1}|]$, если префикс пуст. 

\Abbreviations

ПП --- прямолинейная программа

\Introduction

В настоящее время алгоритмы обрабатывающие большие объемы данных привлекают все больше внимания. Причина не только
в том, что с каждым днем растет объем доступных данных. Так, например, в области машинного обучения привлечение большего
объема данных позволяет увеличить точность получаемой модели. Один из подходов позволяющих снизить скорость
роста размера входных данных заключается в работе со сжатыми представлениями.

Известны различные сжатые представления данных: прямолинейные программы \cite{SLPConstruction} (кратко ПП), коллаж
системы \cite{collages}, представления данных с помощью антисловарей \cite{antidictionaries} и т.д. В настоящее время
сжатие текста с помощью контекстно-свободных грамматик (таких как ПП) является активно развивающимся направлением
научных исследований.  Причина этого не только в том, что грамматики обеспечивают хорошо-структурированное сжатие
данных, но и в том, что сжатие с помощью ПП в некотором смысле полиномиально эквивалентно сжатию данных с помощью
алгоритма Лемпеля-Зива. Другими словами, существует полиномиальная зависимость между размером ПП, выводящей заданный
текст $S$, и размером словаря, построенным алгоритмом Лемпеля-Зива для текста $S$ (подробнее см.
\cite{SLPConstruction}). Заметим, что классические алгоритмы сжатия LZ78 \cite{LZ78} и LZW \cite{LZW} являются частными
случаями грамматического сжатия. При этом другие алгоритмы из семейства алгоритмов Лемпеля-Зива, например LZ77 \cite{LZ77}
и кодирование повторов, не укладываются в модель грамматического сжатия.

Поскольку ПП обеспечивают хорошо-структурированное представление данных, то возникает идея решать классические строковые
задачи в терминах ПП. Существует класс алгоритмов, которые решают следующие строковые задачи в терминах ПП:
\textbf{Поиск образца в тексте} \cite{PM_and_HD}, \textbf{Наибольшая общая подстрока} \cite{LCSubstring}, считающая версия задачи
\textbf{Поиск всех палиндромов} \cite{LCSubstring}, разновидность задачи \textbf{Наибольшая общая подпоследовательность}
\cite{LCS_P}. В тоже время константы, которые скрываются за определением функции $O$ в оценке сложности таких
алгоритмов, как правило очень большие. Также полиномиальная связь между размером ПП, выводящей заданный текст $S$, и
размером LZ77-словаря для этого же текста $S$ еще не гарантирует, что ПП обеспечивают высокую степень сжатия на
практике. Поэтому актуальным становится следующий вопрос: существуют ли модели сжатия с помощью ПП эффективно
работающие на практике? В работе детально рассматриваются два вопроса. Насколько трудно строить ПП на
практике? Насколько высокий уровень сжатия обеспечивают ПП относительно классических алгоритмов сжатия на практике?

\section{Алгоритмы построения ПП}

Классическая формулировка задачи построения ПП выглядит следующим образом:

\problem{Построение ПП}{текст $S$;}{ПП $\slp{S}$, которая выводит $S$;}

Однако, задача построения грамматики минимального размера для заданного текста является NP-трудной \cite{SmallestCFG}.
Поэтому представляют интерес приближенные алгоритмы. Один из основных подходов использует идею факторизации текста.
Суть подхода заключается в том, что для фиксированного разбиения текста алгоритм последовательно просматривает
факторы. Для каждого фактора алгоритм строит ПП, которая выводит этот фактор, а затем добавляет полученную программу
в результирующую ПП. При таком подходе размер результирующей ПП зависит не только от размера исходного текста, но и от 
размера факторизации. Переформулируем задачу построения ПП следующим образом:

\problem{Построение ПП}{текст $S$ и его факторизация $F_1, F_2, \dots, F_k$;}{ПП $\slp{S}$, которая выводит $S$;}

В качестве одного из способов факторизации в \cite{SLPConstruction} рассматривается факторизация, порождаемая алгоритмом
LZ77, так называемая LZ-факторизация.

Если существует полиномиальный алгоритм, решающий задачу построения ПП, то существует полиномиальная связь между
размером построенной ПП и размером факторизации текста. В частности, если на вход дана LZ-факторизация текста,
то существует полиномиальная связь между размером построенной ПП и размером словаря, хранимым алгоритмом LZ77. 

Формулировка задачи построения ПП не накладывает никаких ограничений на дерево вывода ПП. То есть единственное
ограничение -- дерево вывода ПП является бинарным деревом. Существуют различные виды бинарных деревьев. Какой вид больше
подходит для решения этой задачи? В классическом алгоритме построения ПП \cite{SLPConstruction} используются
сбалансированные деревья, а именно, AVL-деревья.

Особенностью AVL-деревьев является строгая логарифмическая оценка на высоту \cite{Knuth}. Это одна из основных причин
почему этот вид деревьев используется в алгоритме Риттера. Но существуют технические трудности с построением
AVL-деревьев. Они будут детально обсуждаться в этой главе. Также в этой главе рассматривается альтернативный алгоритм построения
ПП на основе декартовых деревьев.

Существует вероятностная логарифмическая оценка на высоту декартова дерева (см. раздел Оценка высоты декартова дерева).
Этот класс бинарных деревьев рассматривается потому, что алгоритм построения декартова дерева более простой, чем
AVL-дерева и при этом с большой вероятностью получится дерево логарифмической высоты. Поэтому интересно провести
сравнительный анализ этих подходов на практике.

\subsection{Узкое место алгоритма Риттера}

Следующая теорема дает оценку сверху для размера построенной ПП.

\begin{thm}
Существует алгоритм, который для заданного текста $S$ длины $n$ и его факторизации размера $k$ строит ПП, выводящую
текст $S$, размера $O(k \log n)$ за время $O(k \log n)$.
\end{thm}

Доказательство теоремы содержит алгоритм построения ПП. AVL-сбалансированными грамматиками (кратко AVL-грамматиками)
будем называть такие прямолинейные программы, что их дерево вывода является AVL-деревом. Мы напомним ключевые идеи
алгоритма поскольку они важны для дальнейшей дискуссии. Основной операцией используемой в алгоритме является
конкатенация AVL-грамматик. Следующая лемма дает оценку сверху для этой операции:

\begin{lem}
Пусть $\slp{S}_1, \slp{S}_2$ -- нетерминальные правила AVL-грамматики. Тогда мы можем построить AVL-грамматику $\slp{S}
= \slp{S}_1 \cdot \slp{S}_2$, которая выводит текст $S_1 \cdot S_2$, за время $O(|h(\slp{S}_1) - h(\slp{S}_2)|)$ и
добавив $O(|h(\slp{S}_1) - h(\slp{S}_2)|)$ новых правил.
\end{lem} 

Далее мы представим алгоритм Риттера в общих чертах:

\header{Алгоритм:} Алгоритм строит ПП индуктивно по $k$.

\textbf{База:} Полагаем $\slp{S}$ равной терминальному правилу, выводящему $S[0]$.

\textbf{Шаг:} Предположим, что для фиксированного $i > 1$ уже построена ПП $\slp{S}$, выводящая текст
$\concat{F}{1}{2}{i}$. Так как факторизация текста $S$ фиксирована, то известны позиции вхождения $F_{i+1}$ в тексте
$\concat{F}{1}{2}{i}$. Используя алгоритм взятия подграмматики мы находим правила $\tuple{\slp{S}}{1}{2}{\ell}$ такие,
что $F_{i+1} = \concat{S}{1}{2}{\ell}$. Поскольку $\slp{S}$ AVL-грамматика, то $\ell = O(\log |S|)$. С помощью Леммы 3.2
конкатенируем правила $\tuple{\slp{S}}{1}{2}{\ell}$ в некотором фиксированном порядке (подробнее см.
\cite{SLPConstruction}). Полагаем $\slp{S} = \slp{S} \cdot (\concat{\slp{S}}{1}{2}{i})$.

\begin{figure}
	\caption{Типы вращений узлов AVL-дерева}\label{AVLrotations}
	\begin{center}
		\AVLrotations
	\end{center}
\end{figure}

При работе с AVL-деревьями самой трудной задачей является сохранение баланса в дереве. Например, при добавлении нового
правила в дерево порождается последовательность вращений. Существует два типа вращений, они представлены на Рисунке 1. 
При этом каждое вращение может порождать не более трех новых узлов. Также каждое вращение может породить не более трех
висячих (неиспользуемых) узлов. Таким образом алгоритм большую часть времени проводит за поддержанием баланса.
Существует два направления оптимизации алгоритма: строить более компактные грамматики или минимизировать число запросов
к грамматике. Оптимизация числа запросов к грамматике становится важной в случае, когда размер входного текста превышает
объем оперативной памяти. То есть мы не можем хранить текущее состояние ПП в оперативной памяти. Формально это значит,
что стоимость операции доступа к ПП превышает стоимость вычислений в оперативной памяти.

Следующий пример иллюстрирует узкое место алгоритма Риттера:

\example Пусть $S$ -- текст длины $2^n$, где $n$ -- фиксированное натуральное число. Предположим, что алгоритм
Риттера уже построил ПП $\slp{S}$, которая выводит $S\substr{0}{2^{n-1}}$. Пусть $F_{1}, F_{2}, \dots, F_{n}$ оставшиеся
факторы такие, что $F_{1} \cdot F_{2} \cdot \mbox{\dots} \cdot F_{n} = S\substr{2^{n-1}}{2^{n}}$ и $|F_i| = 2^{n-i-1}$,
где $i \in \{ 1, 2, \dots, n-1 \}$.

Давайте оценим число вращений которое потребуется при последовательной конкатенации грамматик $(((\slp{S} \cdot
\slp{F}_1)\cdot \slp{F}_2)\dots) \cdot \slp{F}_{n-1}$ в худшем случае. Чтобы получить оценку сверху, мы предполагаем,
что после конкатенации двух правил высота результирующего дерева увеличивается на 1. 
\begin{multline*} 
\sum_{i=1}^{n-1} (\log{2^{n-1}} + (i-1) - \log{2^{n-i-1}}) = \frac{(n-1)(n-2)}{2}(1 + \log{2}) - (n-2) = \Theta(n^2).
\end{multline*}

Заметим, что если бы алгоритм мог выбирать порядок конкатенации грамматик, то число вращений порождаемых алгоритмом
могло быть существенно меньше. Например, если выполнить конкатенацию грамматик в обратном порядке $\slp{S} \cdot
(\slp{F}_1 \dots (\slp{F}_{n-2} \cdot \slp{F}_{n-1}))$, то получится следующая оценка:
$$\sum_{i=1}^{n-1} (\log{2^{n-i}} - \log{2^{n-i-1}}) = (n-1)\log{2} = \Theta(n).$$

Следующий пример показывает, что несколько факторов могут быть обработаны вместе, если они входят в тексте выводимом из
одной ПП.

\example Пусть $S = b\cdot a^{2^{n-1}}\cdot b \cdot a^{2^{n-2}} \cdot \mbox{\dots} \cdot b \cdot a$, где $n$ --
фиксированное натуральное число и $|S| = 2^n + n - 2$. Рассмотрим LZ-факторизацию $S$: 
$$b \cdot a \cdot a \cdot a^2 \cdot a^4 \cdot \mbox{\dots} \cdot a^{2^{n-2} - 1} \cdot ba^{2^{n-2}} \cdot ba^{2^{n-3}}
\cdot \mbox{\dots} \cdot ba.$$

Пусть $\slp{S}_1$ -- ПП, выводящая текст $b \cdot a^{2^{n-1}}$. Очевидно, что все факторы, начиная с $b \cdot
a^{2^{n-2}}$, входят в $S_1$. Следовательно, не имеет значения в каком порядке они будут обработаны алгоритмом.
Алгоритм мог бы построить ПП $\slp{S}_2, \slp{S}_3, \dots, \slp{S}_{n-1}$ выводящие тексты $ba^{2^{n-3}}, ba^{2^{n-4}}, \dots, ba$
соответственно. Наконец алгоритм мог бы конкатенировать полученные ПП в следующем порядке:
$\slp{S}_1\cdot(\dots(\slp{S}_{n-3}\cdot(\slp{S}_{n-2} \cdot \slp{S}_{n-1})))$.

Идея оптимизации заключается в том, чтобы обрабатывать факторы максимально возможными группами и в рамках каждой группы
факторов выбирать оптимальный порядок конкатенации. Интуитивно, идея очень проста: если алгоритм уже построил большую
ПП, то большинство факторов входит в тексте выводимом из нее, а значит факторы могут быть обработаны вместе.

\subsection{Оптимизация алгоритма Риттера}

\noindent \textsc{Вход:} текст $S$ и его LZ-факторизация $\tuple{F}{1}{2}{k}$.

\noindent \textsc{Выход:} ПП $\slp{S}$, которая выводит текст $S$.

\noindent \textsc{Алгоритм:} Алгоритм строит ПП по индукции.

\textbf{База:} Полагаем $\slp{S}$ равным терминальному правилу, выводящему $S[0]$. 

\textbf{Шаг:} Зафиксируем произвольное число $0 < i < k$. Пусть $\slp{S}$ -- ПП, которая выводит $\tuple{F}{1}{2}{i}$.
Пусть $1 \leq \ell \leq k - i$ -- наибольшее число такое, что $F_{i+\ell}$ входит в $\concat{F}{1}{2}{i}$. Так как
LZ-факторизация $S$ фиксирована, тогда значение $\ell$ может быть найдено с помощью линейного поиска по факторам. ПП
$\tuple{\slp{F}}{i+1}{i+2}{i+\ell}$, которые выводят тексты $\tuple{F}{i+1}{i+2}{i+\ell}$ соответственно, вычисляются с
помощью алгоритма взятия подграмматики (аналогично алгоритму из \cite{SLPConstruction}).

Алгоритм вычисляет конкатенацию ПП $\tuple{F}{i+1}{i+2}{i+\ell}$ с помощью динамического программирования. Положим
$\varphi(p, q)$ -- функция, которая вычисляет число операций вращения AVL-дерева при конкатенации
$\tuple{\slp{F}}{p}{p+1}{q}$. Алгоритм использует следующую формулу для функции $\varphi$:

$$\varphi(p, q) = \begin{cases}
\qquad \qquad \qquad \qquad \qquad 0 &\text{если } p=q, \\
\qquad \qquad \min_{r = p}^q(\varphi(p, r) + \varphi(r+1, q) +&\\
|\log(|F_{i+p}| + \mbox{\dots} + |F_{i+r}|) - \log(|F_{i+r+1}| + \mbox{\dots} + |F_{i+q}|)|) &\mbox{иначе}.
\end{cases}$$

Формула корректна, так как конкатенация двух правил $\slp{F}_p$ и $\slp{F}_q$ порождает не более $2\cdot|h(\slp{F}_p) -
h(\slp{F}_q)|$ вращений AVL-дерева (см. \cite{SLPConstruction}) и $h(\slp{F}_p) \leq 1.44 \cdot \log |F_p|$ (см.
\cite{Knuth}). Для простоты общие константы были отброшены из формулы.

Вычисленные значения функции $\varphi(p, q)$ хранятся в $\varphi$-таблице. $\varphi$-таблица это квадратная таблица
размера $(\ell-1) \times (\ell-1)$, которая хранит информацию о значении $\varphi(p, q)$ и информацию об оптимальном
разбиении. Ячейки таблицы в $p$-й строке и $q$-м столбце по умолчанию заполнены нулями, если $p \leq q$. Алгоритм
заполняет таблицу в следующем порядке: сначала заполняются ячейки $(p, q)$ такие, что $q - p = 1$, потом заполняются
ячейки $(p, q)$ такие, что $q - p = 2$ и т.д. Используя такой порядок заполнения алгоритм избегает рекурсивного
пересчета значений $\varphi(p, q)$, так как они уже содержаться в таблице.

Любое значение $\varphi(p, q)$ может быть вычислено за время $O(\ell)$ (псевдокод процедуры представлен на Рисунке
\ref{ComputingFiValue}). В итоге алгоритм заполнит $\varphi$-таблицу за время $O(\ell^3)$ и использует $O(\ell^2)$ памяти.

\begin{figure}
	\caption{Псевдокод функции, вычисляющей значение $\varphi(p,q)$}\label{ComputingFiValue}
	\begin{center}
		\ComputingFiValue
	\end{center}
\end{figure}

Заметим, что значение $\varphi(1, \ell)$ оценивает сверху количество операций перебалансировки, которое необходимо
выполнить для конкатенации последовательности ПП $\tuple{\slp{F}}{i+1}{i+2}{i+\ell}$. Поскольку в каждой ячейке
$\varphi$-таблицы хранится индекс оптимального разбиения, то алгоритм за время $O(\ell)$ может определить оптимальный порядок конкатенации ПП
$\tuple{\slp{F}}{i+1}{i+2}{i+\ell}$. Используя этот порядок алгоритм строит ПП $\slp{F}$, которая выводит текст
$\concat{F}{i+1}{i+2}{i+\ell}$. В итоге алгоритм полагает ПП $\slp{S}$ равной $\slp{S} \cdot \slp{F}$.

\begin{thm}
Пусть $\tuple{F}{1}{2}{k}$ -- LZ-факторизация текста $w$, тогда представленный выше алгоритм (оптимизрованная версия
алгоритма Риттера) построит ПП, выводящую текст $w$, размера $O(k \log n)$.
\end{thm}  

\proof Разобьем LZ-факторизацию на независимые группы с помощью жадного алгоритма. Пусть $\tuple{G}{1}{2}{p}$ --
разбиение LZ-факторизации, тогда $G_1 = \{F_1\}$ и для любого $1 < j \leq p$ $G_j = \{\tuple{F}{i_j}{i_j+1}{i_j +
\ell_j}\}$, где $F_{i_j}$ входит в $\concat{F}{i_{j-1}}{i_{j-1}+1}{i_{j-1} + \ell_{j-1}}$ и не входит в
$\concat{F}{1}{2}{i_{j-2} + \ell_{j-2}}$; каждый фактор $\tuple{F}{i_j + 1}{i_j + 2}{i_j + \ell_j}$ входит в
$\concat{F}{1}{2}{i_{j-1} + \ell_{j-1}}$. $\tuple{\ell}{1}{2}{p}$ -- размеры соответствующих групп факторов и
$\sum_{j=1}^{p} \ell_j = k$. 

Число операций вращений необходимое при работе с $j$-ой группой факторов ограничено сверху $2.88 \cdot (\log |F_{i_j}| +
\log |F_{i_j + 1}| + \dots + \log |F_{i_j + \ell_j}|) \cdot \ell_j \leq 2.88 \cdot \log n \cdot \ell_j$. Суммируем по
параметру $j$ и получаем, что общее число операций вращения необходимое для построения ПП ограничено сверху $2.88 \cdot
k \cdot \log n$. Поскольку каждая операция вращения порождает не более трех новых правил, тогда размер ПП будет порядка
$O(k \log n)$

Так как мы используем алгоритм динамического программирования для вычисления порядка конкатенации, то мы не можем
оценить сложность алгоритма в зависимости от входных параметров. Заметим, что если размеры всех групп факторов равны 1,
то мы получаем в точности алгоритм Риттера \cite{SLPConstruction}. Ясно, что новый алгоритм порождает меньше вращений
чем классический алгоритм, с другой стороны новый алгоритм использует дополнительные ресурсы для вычисления порядка
конкатенации. В следующей главе мы сравним эффективность обоих алгоритмов на практике.

\subsection{Построение ПП на основе декартовых деревьев} 

Алгоритмы построения ПП тесно связаны со структурой дерева вывода. Например, в случае AVL-деревьев нам приходится
тратить время на поддержание баланса в дереве. Поэтому возникает идея, что со сменой структуры данных для представления
дерева вывода может получиться принципиально другой алгоритм построения ПП. В качестве альтернативной структуры данных
в этой работе рассматриваются декартовы деверья.

\begin{lem}
Для заданного набора ключей $(x_1, y_1)$, \ldots, $(x_n, y_n)$ существует единственное декартово дерево, содержащее
этот набор ключей при условии, что все ключи $x_i$ попарно различны и все ключи $y_i$ попарно различны.
\end{lem}

\proof Докажем индукцией по параметру $n$. 

\noindent \textbf{База:} Для $n = 0$ существует единственное пустое дерево.

\noindent \textbf{Шаг:} Зафиксируем произвольное число $0 < k < n$. Пусть для любого набора ключей мощности
меньше $k$ существует единственное декартово дерево, их содержащее. Докажем это утверждение для набора мощности $k$.
Пусть $root$ -- такой индекс, что $y_{root} = \min_{i=1}^k{y_i}$, тогда множество ключей разбивается на два множества 
$K_{left} = \{(x_i, y_i) | x_i < x_{root}\}$ и $K_{right} = \{(x_i, y_i) | x_i > x_{root}\}$. По свойствам кучи
$(x_{root}, y_{root})$ -- ключи корня искомого дерева. По свойствам бинарного дерева поиска левое и правое 
поддеревья искомого дерева построены по множествам ключей $K_{left}$ и $K_{right}$ соответственно. Так как ключи попарно
различны, то значения $(x_{root}, y_{root})$, $K_{left}$ и $K_{right}$ определяются однозначно, по предположению индукции левое и
правое поддеревья строятся однозначно.

\textbf{Следствие.} В силу единственности декартова дерева, содержащего данный набор ключей, все его характеристики
(например, высота) однозначно определяются набором ключей и не зависит от способов построения самого декартового дерева.

Рассмотрим операции над декартовыми деревьями. Существует две стандартные операции над декартовыми деревьями:
\emph{split} и \emph{merge}.

\emph{merge} -- бинарная операция над декартовыми деревьями, результатом которой является дерево, содержащее все ключи
деревьев-операндов. Причем на операнды наложено условие, что любой ключ $x$ первого дерева меньше любого ключа $x$
второго дерева. Рассмотрим алгоритм, который выполняет эту операцию:

\textsc{Операция:} \emph{merge}

\textsc{Вход:} Два декартовых дерева $T_1$, $T_2$. 

\textsc{Выход:} Декартово дерево $T$, содержащее все ключи из деревьев $T_1$ и $T_2$.

\textsc{Алгоритм:}

\begin{enumerate}
  \begin{item}
	Если одно из деревьев пустое, то результатом будет другое дерево.
  \end{item}
  \begin{item}
	Пусть оба дерева непустые и положим $T_1 =$ ($L_1$, $R_1$, $x_1$, $y_1$), $T_2 =$ ($L_2$, $R_2$, $x_2$, $y_2$). Из
	свойств кучи получаем, что $y_1$ -- минимальный ключ $y$ в дереве $T_1$, $y_2$ -- в $T_2$. Поэтому корнем
	дерева-результата будет один из корней деревьев-операндов.
	\begin{enumerate}
		\begin{item}
			Если $y_1 < y_2$, то построим дерево $T_3 = merge(R_1, T_2)$. Тогда результатом будет дерево $T =$ ($L_1$, $T_3$,
			$x_1$, $y_1$).
		\end{item}
		\begin{item}
			Случай $y_1 \geq y_2$, аналогичен предыдущему.
		\end{item}
	\end{enumerate}
  \end{item} 
\end{enumerate}

\begin{lem}
Если $h_1$ и $h_2$ -- высоты деревьев $T_1$ и $T_2$ соответственно, то операция \emph{merge} для этих двух деревьев
работает за время $O(h_1 + h_2)$.
\end{lem}

\proof Заметим, что если деревья непустые, то алгоритм делает ровно один рекурсивный вызов, при этом высота одного из
деревьев уменьшается как минимум на $1$. 

\emph{split} -- бинарная операция над декартовым деревом и числом (позиция разреза), результатом которой является упорядоченная пара декартовых деревьев,
где ключи $x$ первого дерева меньше позиции разреза, а ключи $x$ второго дерева не меньше позиции разреза.

Рассмотрим алгоритм, который выполняет эту операцию:

\textsc{Операция:} \emph{split}

\textsc{Вход:} Декартово дерево $T$ и $x_{split}$ -- позиция разреза. 

\textsc{Выход:} Упорядоченная пара декартовых деревьев ($L$, $R$) такая, что $x_i < x_{split}$ для всех $x_i \in L$ и
$x_j \geq x_{split}$ для всех $x_j \in R$.

\textsc{Алгоритм:}

\begin{enumerate}
  \begin{item}
	Если дерево $T$ пустое, то результатом будет два пустых дерева.
  \end{item}
  \begin{item}
	Пусть $T$ непустое дерево и положим $T = $($LT$, $RT$, $x$, $y$).
	\begin{enumerate}
		\begin{item}
			Если $x_{split} < x$, то необходимо разрезать левое поддерево $(L', R')  = split(LT, x_{split})$.
			Тогда результатом будет ($L$, $R$), где $L = L'$, $R = $($R'$, $RT$, $x$, $y$).
		\end{item}
		\begin{item}
			Случай $x_{split} \geq x$ аналогичен предыдущему.
		\end{item}
	\end{enumerate}
  \end{item} 
\end{enumerate}

\begin{lem}
Пусть $h$ -- высота декартова дерева $T$, тогда операция \emph{split} работает за время $O(h)$ независимо от позиции
разреза.
\end{lem}

\proof Пусть $T$ непустое дерево, тогда алгоритм делает ровно один рекурсивный вызов для текущего поддерева, при этом
высота уменьшается как минимум на $1$.

В итоге мы получаем, что сложность операций с декартовым деревом линейно зависит от его высоты. В действительности
высота декартова дерева может быть линейной относительно его размера. Например, высота декартова дерева, построенного по
набору ключей ($1$, $1$), \ldots, ($n$, $n$), будет равна $n$. В следующем разделе мы докажем, что декартово дерево из
$n$ узлов, ключи $y$ которых являются независимыми непрерывными случайными величинами с одинаковым вероятностным
распределением, имеет высоту $O(\log n)$.

\subsubsection{Оценка высоты декартова дерева}

В этом разделе мы предполагаем, что все ключи $x$ пронумерованы в соответствии с линейным порядком, то есть $x_1 <
x_2 < \ldots x_n$, и каждое $y_i$ выбрано случайно и независимо с одинаковым распределением.

Следующее наблюдение является следствием упорядоченности ключей $y$ в декартовом дереве.

\begin{lem}
$i$-й узел является предком $k$-го узла тогда и только, когда $y_i < y_j$ для любого $j$ такого, что $i < j	\leq k$ или
$k \leq j < i$.
\end{lem}

\proof Рассмотрим случай $i < k$, в случае $i > k$ доказательство аналогично.

\textbf{Необходимость.} Допустим, что $y_i < y_j$ для всех $i < j \leq k$. Тогда по свойствам кучи $i$-й узел не может
лежать в поддереве с корнем в $k$-м узле. Более того, не может существовать индекс $j$ такой, что $j$-й узел общий прародитель
$i$-го и $k$-го узлов, но эти узлы лежат в его разных  поддеревьях. Если он существует, то $x_i < x_j < x_k$,
следовательно, $i < j < k$. Но тогда $j$-й не мог быть прародителем $i$-го узла. Остался последний вариант взаимного
расположения $i$-го и $k$-го узлов~--- $i$-й является прародителем $k$-го узла.

\textbf{Достаточность.} Пусть $i$-й узел является предком $k$-го узла. Докажем от противного. Предположим, что
существует $j$ такое, что $i < j \leq k$ и $y_j < y_i$. По свойствам кучи $i$-й узел не может быть прародителем $j$-го узла.
Если $j$-й узел является прародителем $i$-го узла, то из неравенства $x_i < x_j < x_k$ следует, что $i$-й узел
содержится в левом поддереве $j$-го узла, а $k$-й~--- в правом. Это противоречит тому, что $i$-й узел прародитель $k$-го узла. Остается последний вариант взаимного расположения
$i$-го и $j$-го узла~--- некоторый $l$-й узел является их общим прародителям, но они содержатся  в его разных
поддеревьях. Тогда получаем неравенство $x_i < x_l < x_j \leqslant x_k$, из которого следует, что $k$-й узел содержится
в правом поддереве $l$-го узла. Получаем противоречие. Следовательно, наше предположение не верно.

\begin{lem}
	Математическое ожидание глубины $k$-го узла равно $H_k + H_{n-k+1} - 1$, где $H_j = \sum_{i=1}^j\frac{1}{i} \approx
	\ln j$.
\end{lem}

\proof Пусть $A_i$ -- случайная величина такая, что ее значение равно единице, если $i$-й узел является предком
$k$-го узла, иначе равно нулю. Ясно, что $A_i$ и $A_j$ независимы при $i \ne j$. Поскольку $y_i$ выбраны случайно и
независимо с одинаковым распределением, тогда из предыдущего наблюдения следует, что $M(A_i) = \frac{1}{|i - k| + 1}$
при $i \ne k$.

%TODO: тут точно баг

$$M(depth_k) = M(\sum_{i=1}^nA_i) = \sum_{i=1}^nM(A_i) = \sum_{i=1}^n\frac{1}{|i-k|+1} = $$

$$= \sum_{i=1}^k\frac{1}{k-i+1} + \sum_{i=k}^n\frac{1}{i-k+1} - 1 = H_k + H_{n-k+1} - 1.$$

Следовательно, математическое ожидание глубины конкретного узла будет порядка $O(\log n)$.

Для доказательства оценки высоты декартова дерева нам потребуется следующее вспомогательное утверждение:

\begin{lem}[\cite{ChernoffBound}]
Рассмотрим независимые случайные величины $A_1$, $A_2$ \ldots $A_n$. Тогда если $A = \sum_{i=1}^nA_i$ и
$\mu = M(A)$, то для любого $\delta > 0$

$$
	P\{A > (1 + \delta)\mu\} < \left(\dfrac{e^{\delta}}{(1 + \delta)^{(1 + \delta)}}\right)^{\mu}
$$
\end{lem}

\begin{lem}
	$P\left\{depth_k \leqslant 7(H_k + H_{n - k + 1} - 1)\right\} \geqslant 1 - (\frac{1}{n})n^6$.
\end{lem}

\proof Используя Лемму 3.5 получаем $M(depth_k) = H_k + H_{n-k+1} - 1$. Тогда применяя Лемму 3.6 получаем что для любого
$\delta > 0$ выполнено следующее неравенство:

$$
P\{A > (1 + \delta)\mu\} < \left(\dfrac{e^{\delta}}{(1 + \delta)^{(1 + \delta)}}\right)^{\mu}
$$

где $\mu = H_k + H_{n-k+1} - 1$. Заметим, что $H_k + H_{n - k + 1} - 1 \leqslant H_n \leqslant \ln n$ для любого $k$.
Тогда подставив $\delta = e^2 - 1 > 6$ получаем следующую оценку:

$$	P\{A > (1 + \delta)(H_k + H_{n-k+1} - 1)\} < \left(\frac{e^{\delta}}{(1 + \delta)^{(1 + \delta)}}\right)^{H_n}$$
$$	< \left(\frac{e^{\delta + 1}}{(e^2)^{(1 + \delta)}}\right)^{H_n} < \left(\frac{1}{e}\right)^{\delta H_n} <
	\left(\frac{1}{n}\right)^6 $$

\begin{thm} Декартово дерево, построенное по набору ключей ($x_1$, $y_1$), \ldots, ($x_n$, 	$y_n$), где $y_i$ выбраны
случайно, независимо и имеют одинаковое непрерывное распределение, имеет высоту $O(\log n)$.
\end{thm}

\proof Пусть $B_i$~--- событие, при котором $depth_i > 7(H_k + H_{n - k + 1} - 1)$.
Тогда оценим вероятность объединения этих событий, то есть вероятность того, что глубина дерева
будет больше чем $7(H_k + H_{n - k + 1} - 1)$.

$$
	P\{B_1 \cup \ldots \cup B_n\} \leqslant P\{B_1\} + \ldots + P\{B_n\} \leqslant n \cdot \left(\frac{1}{n}\right)^6 =
	\left(\frac{1}{n}\right)^5 
$$

Таким образом, вероятность того, что дерево глубина дерева будет больше чем $7(H_k + H_{n - k + 1} - 1)$ не превосходит
$\frac{1}{n^5}$. Иначе говоря, с вероятностью $1 - \frac{1}{n^5}$ глубина декартова дерева будет $O(\log n)$.
Поскольку глубина дерева ограничена сверху числом $n$,тогда математическое ожидание высоты декартова дерева со случайными ключами
$y$ равна $O(\log n)$.

\textbf{Следствие 1.} Ключи $y$ для декартова дерева можно не хранить. Достаточно случайно их генерировать по мере
необходимости. При этом логарифмическая оценка на высоту дерева сохранится.

\textbf{Следствие 2.} Операции \emph{merge} и \emph{split} для деревьев со случайными ключами $y$ работает за $O(\log
n)$.

Так как распределение является непрерывным, то ключи $y$ являются попарно различными. Следовательно, для таких деревьев
выполняется утверждение о единственности дерева.

\subsubsection{Класс декартовых деревьев, подходящий для построения ПП}

Для того, чтобы построить ПП требуются две операции над деревом вывода: операция взятия поддерева по заданным границам и
операция конкатенации двух деревьев. Аналогичные операции существуют для декартовых деревьев. А именно, \emph{split}
-- операция разбиения декартова дерева на два поддерева по заданной границе и \emph{merge} -- операция слияния двух
декартовых деревьев. В классической реализации операции \emph{merge} требуется дополнительное условие: ключи $x$ первого
дерева должны быть меньше ключей $x$ второго дерева. Более того, операция \emph{merge} не гарантирует сохранения порядка
листов после ее применения. Следовательно, чтобы использовать декартовы деревья в алгоритмах построения ПП необходимо
преодолеть следующие трудности:

\begin{itemize}
  \item Корректная перегенерация ключей для деревьев, полученных после применения операции \emph{split}. Эта
  задача возникает в следующем случае: пусть алгоритм уже построил некоторое дерево $T$, для следующего фактора алгоритм
  вырезает поддерево $T'$ дерева $T$ и необходимо выполнить операцию \emph{merge} для $T$ и $T'$. Согласно требованиям
  операции \emph{merge} необходимо полностью перегенерировать ключи $x$ для дерева $T'$. 
  \item Реализовать операцию \emph{merge} так, чтобы гарантировать порядок листов в результирующем дереве. Так как
  необходимо построить ПП, которая выводит в точности исходный текст.
\end{itemize}

Пусть для произвольного декартова дерева была потеряна информация о его ключах $x$. Тогда по структуре дерева всегда
можно восстановить отношение линейного порядка на ключах $x$. Например, обойдем рекурсивно дерево в следующем порядке:
левое поддерево, корень, правое поддерево. Дополнительно в каждом узле $T_i$ будем хранить $count(T_i)$ -- количество
узлов в поддереве с корнем в $T_i$. Тогда мы сможем восстановить порядковый номер ключа $x$ в данном узле среди ключей
$x$ из узлов данного поддерева (это количество узлов в левом поддереве плюс один). Таким образом можно отказаться от
явного хранения значения ключей, сохраняя лишь их порядок. Рассмотрим класс декартовых деревьев по неявному
ключу в класса деревьев на основе которых мы будем строить ПП. Для этого класса деревьев справедливы следующие
утверждения:

\begin{lem}
Для заданного набора ключей $(y_1, y_2, \ldots y_n)$ существует единственное декартово дерево по неявному ключу,
содержащее его при условии, что все ключи $y_i$ попарно различны.
\end{lem}

\proof Докажем от противного. Предположим, что существует два различных декартовых дерева по неявному ключу,
построенных по заданному кортежу ключей $y$. Рассмотрим естественные декартовы деревья для них. Очевидно, что эти
деревья будут различными. Но они содержат один и тот же набор ключей. Это противоречит теореме о единственности
декартова дерева. Значит, наше предположение неверно и существует единственное декартово дерево по неявному ключу, построенное
по заданному кортежу ключей $y$.

\begin{thm}
Декартово дерево по неявному ключу, построенное по набору ключей $(y_1, y_2 \ldots y_n)$, где $y_i$ выбраны случайно и
независимо с одинаковым вероятностным распределением, имеет высоту $O(\log n)$.
\end{thm}

\proof Восстановим из декартова дерева по неявному ключу обычное декартово дерево. По построению их структуры совпадают.
Следовательно и высоты этих деревьев совпадают. В силу теоремы для декартовых деревьев высота этих деревьев $O(\log n)$.

В дальнейшем под значением ключа $x$ для узла $T_i \in T$ мы будем подразумевать порядковый номер $T_i$ в отношении
линейного порядка ключей $x$ для $T$. 

С помощью декартовых деревьев с неявным ключом решается вопрос о перегенерации ключей $x$. Также если в листах такого
дерева хранить дополнительную информацию (например некоторый символ), то можно организовать динамический массив
символов. Другими словами операция \emph{merge} сохраняет порядок листов. Поскольку декартовы деревья с неявным ключом
всего лишь подкласс всех декартовых деревьев, то для этого подкласса сохраняется логарифмическая оценка высоты.

Рассмотрим операции, которые необходимы для построения ПП:

\header{Операция:} \emph{split}

\header{Вход:} $T$ -- декартово дерево с неявным ключом, $0 < k \leq |T|$ -- число; 

\header{Выход:} $L, R$ -- пара декартовых деревьев с неявным ключом такие, что $L$ содержит все узлы $T$ с ключами
меньшими $k$, а $R$ содержит все остальные узлы $T$;

\header{Алгоритм:} 

Алгоритм работает рекурсивно, начиная с корня дерева $T$. Пусть в текущий момент алгоритм
просматривает узел $T_i = (T_\ell, T_r)$. Рассмотрим следующие случаи:

\begin{itemize}
  \item Если $k = count(T_\ell) + 1$, то $L = T_\ell, R = T_r$.
  \item Если $k < count(T_\ell) + 1$, то необходимо разрезать $T_\ell$. Пусть $(L', R') = split(T_\ell, k)$, тогда $L = L', R =
  (R', T_r)$.
  \item Если $k > count(T_\ell) + 1$, то необходимо разрезать $T_r$. Пусть $(L', R') = split(T_r, k - count(T_\ell) - 1)$,
  тогда $L = (T_\ell, L'), R = R'$.
\end{itemize}

\header{Сложность:} 

На каждом шаге рекурсии алгоритм либо завершает работу, либо выполняет рекурсивный вызов с узлом
меньшей высоты. Следовательно, сложность алгоритма $O(\log |T|)$.

\vspace{5pt}

\header{Операция:} \emph{merge}

\header{Вход:} $T_1, T_2$ -- декартовы деревья с неявным ключом; 

\header{Выход:} $T$ -- декартово дерево с неявным ключом, содержащее все узлы из $T_1$ и $T_2$;

\header{Алгоритм:} 

Алгоритм работает рекурсивно, начиная с корней деревьев $T_1$ и $T_2$. Пусть в текущий момент
алгоритм просматривает узлы $T_i = (T_{\ell_1}, T_{r_1}) \in T_1, T_j = (T_{\ell_2}, T_{r_2}) \in T_2$. Сгенерируем
произвольное число $y^*$ от 1 до $count(T_i) + count(T_j) + 1$.

\begin{itemize}
  \item Если $0 < y^* \leq count(T_i)$, то рекурсивно построим $T' = merge(T_{r_1}, T_2)$ и возвращаем $(T_{\ell_1}, T')$.
  \item Если $count(T_i) < y^* \leq count(T_j)$, то рекурсивно строим $T' = merge(T_1, T_{\ell_2})$ и возвращаем $(T',
  T_{r_2})$.
  \item Если $y^* = count(T_i) + count(T_j) + 1$, то возвращаем $(T_i, T_j)$.
\end{itemize}

Заметим, что результатом слияния декартова дерева $T$ и пустого дерева является дерево $T$.

\header{Сложность:} 

На каждом шаге рекурсии алгоритм спускается либо внутри первого дерева, либо внутри второго дерева. 
Следовательно, сложность алгоритма $O(\log |T_1| + \log |T_2|)$.

В итоге для реализации всех необходимых операций нам потребовалось лишь дополнительно хранить информацию о количестве
узлов в декартовом дереве.

\subsubsection{Алгоритм построения ПП на основе декартовых деревьев}

В этом разделе под ПП мы будем понимать прямолинейную программу такую, что ее дерево вывода является декартовым деревом
по неявному ключу.

Переформулируем операции $split$ и $merge$ в терминах ПП и докажем их корректность.

\header{Операция:} \emph{split}

\header{Вход:} ПП $\slp{S}$, которая выводит текст $S$, $0 < k < |S|$ -- число; 

\header{Выход:} ПП $\slp{L}, \slp{R}$ такие, что $\slp{L}$ выводит $S\substr{0}{k}$, $\slp{R}$ выводит
$S\substr{k+1}{|S|}$;

\header{Алгоритм:} Аналогичен алгоритму для декартовых деревьев по неявному ключу.

\header{Корректность:} 

Докажем корректность индукцией по $|S|$.

\textbf{База:} Пусть $\slp{S} = \slp{S}_\ell \cdot \slp{S}_r$ и $|S| = 2$, тогда $k = 1$. Очевидно, что правила
$\slp{S}_\ell, \slp{S}_r$ являются терминальными. Следовательно, алгоритм вернет корректный результат.

\textbf{ Шаг:} Пусть для всех $\slp{S}_i$ таких, что $|S_i| < k$ алгоритм корректен. Докажем, что алгоритм
корректно работает для любого правила $\slp{S}' = \slp{S}_\ell \cdot \slp{S}_r$, где $|S_\ell| < k, |S_r| < k$ и $|S'| > k$.

\begin{enumerate}
	\begin{item}
		Пусть $k = |S_\ell|$, тогда алгоритм корректен так как $\slp{S}_\ell, \slp{S}_r$ -- ПП такие, что их деревья вывода
		являются декартовыми деревьями по неявному ключу и $|S_\ell| = k, |S_r| = |S'| - k$.
	\end{item} 
	\begin{item}
		Пусть $k > |S_\ell|$, тогда по предположению индукции вызов $split$ с параметрами $\slp{S}_r$ и $k - |S_\ell|$ вернет
		корректный результат. Пусть $(\slp{L}', \slp{R}') = split(\slp{S}_r, k - |S_\ell|)$, где $\slp{L}'$ выводит текст
		$S_r\substr{0}{k - |S_\ell|}$, а $\slp{R}'$ выводит текст $S_r\substr{k-|S_\ell|}{|S_r|}$, тогда ПП $\slp{S}_\ell \cdot
		\slp{L}'$ выводит текст $S'\substr{0}{|S_\ell|} \cdot S'\substr{|S_\ell|}{k} = S'\substr{0}{k}$, а $\slp{R}'$ выводит текст
		$S'\substr{k}{|S'|}$. Отсюда следует корректность алгоритма.
	\end{item}
	\begin{item}
		Случай $k < |S_\ell|$ аналогичен предыдущему случаю.
	\end{item}
\end{enumerate}

\vspace{5pt}

\header{Операция:} \emph{merge}

\header{Вход:} ПП $\slp{S}_1, \slp{S}_2$, которые выводят тексты $S_1$ и $S_2$ соответственно; 

\header{Выход:} ПП $\slp{S}$, которая выводит текст $S_1 \cdot S_2$;

\header{Алгоритм:} Аналогичен алгоритму для декартовых деревьев по неявному ключу.

\header{Корректность:}

Докажем корректность индукцией по суммарному количеству узлов в деревьях вывода $\slp{S}_1$ и $\slp{S}_2$.

\textbf{База:} Пусть $|\slp{S}_1| = |\slp{S}_2| = 1$, тогда оба корня являются терминальными правилами и 
$count(\slp{S}_1) = count(\slp{S}_2) = 1$. Следовательно, $\slp{S}_1 \cdot \slp{S}_2$ искомая ПП.

\textbf{Шаг:} Зафиксируем числа $1 < k_1 < |\slp{S}_1|, 1 < k_2 < |\slp{S}_2|$. Пусть для всех ПП $\slp{S}_i, \slp{S}_j$
таких, что $|\slp{S}_i| < k_1, |\slp{S}_j| < k_2$ алгоритм корректно выполняет операцию $merge$. Докажем корректность
операции $merge$ для двух ПП $\slp{S}_p = \slp{S}_{\ell_1} \cdot \slp{S}_{r_1}, \slp{S}_q = \slp{S}_{\ell_2} \cdot
\slp{S}_{r_2}$, где $|\slp{S}_{\ell_1}| < k_1, |\slp{S}_{r_1}| < k_1, |\slp{S}_{\ell_2}| < k_2, |\slp{S}_{r_2}| < k_2$ и
$|\slp{S}_p| > k_1, |\slp{S}_q| > k_2$. Рассмотрим следующие возможные случаи:

\begin{enumerate} 
	\begin{item}
		Пусть $0 < r \leqslant k_1$. Предположению индукции все ПП $\slp{S}_{\ell_1}, \slp{S}_{r_1}, \slp{S}_{\ell_2},
		\slp{S}_{r_2}$ являются непустыми и вызов функции $merge(\slp{S}_{r_1}, \slp{S}_q)$ корректен. То есть $\slp{S}^* =
		merge(\slp{S}_{r_1}, \slp{S}_q)$ является ПП, которая выводит текст $S^* = S_{r_1} \cdot S_q$ и ее дерево вывода
		является декартовой ПП с неявным ключом. Следовательно, ПП $\slp{S}_{\ell_1} \cdot \slp{S}^*$ является ПП, которая
		выводит текст $S_{\ell_1} \cdot S^* = S_p \cdot S_q$ и ее дерево вывода является декартовой ПП с неявным ключом.
	\end{item}
	\begin{item}
		Случай $k_1 < r \leqslant k_1 + k_2$ аналогичен предыдущему.
	\end{item}
	\begin{item}
		Пусть $r = k_1 + k_2 + 1$, тогда очевидно, что $\slp{S}_p \cdot \slp{S}_q$ выводит текст $S_p \cdot S_q$ и дерево
		вывода этой ПП является декартовым деревом с неявным ключом.
	\end{item}
\end{enumerate}

В итоге получаем следующий алгоритм построения ПП:

\header{Вход:} текст $S$ и его LZ-факторизация $\tuple{F}{1}{2}{k}$.

\header{Выход:} ПП $\slp{S}$, которая выводит текст $S$.

\header{Алгоритм:} 

Алгоритм строит ПП по индукции.

\textbf{База:} Полагаем $\slp{S}$ равным терминальному правилу, выводящему $S[0]$. 

\textbf{Шаг:} Зафиксируем произвольное число $0 < i < k$. Пусть $\slp{S}$ -- ПП, которая выводит $\tuple{F}{1}{2}{i}$.
Так как LZ-факторизация $S$ фиксирована, тогда мы знаем позиции $\ell, r$ начала и конца вхождения $F_{i+1}$ внутри
$\concat{F}{1}{2}{i}$ соответственно. Построим $\slp{F}_{i+1}$ следующим образом: $\slp{F}_{i+1} = split(split(\slp{S},
\ell)[1], r-\ell)[0]$. Наконец положим $\slp{S} = merge(\slp{S}, \slp{F}_{i+1})$.

\header{Сложность:} Всего в алгоритме $k$ шагов. На каждом шаге запускается не более двух раз операция $split$
и не более одного раза операция $merge$. Следовательно, сложность каждого шага алгоритма $O(\log n)$. В итоге получаем,
что сложность алгоритма построения ПП на основе декартовых деревьев $O(k \log n)$. Размер декартова дерева будет $O(k
\log n)$.

\section{Практические результаты}

\subsection{Условия проведения практических исследований}

Очевидно, что природа исходного текста сильно влияет на время и степень сжатия. Поэтому в работе проводятся исследования
на следующих типах текста:

\begin{itemize}
  \item последовательности ДНК, взятые из открытого банка ДНК Японии \newline (\url{http://www.ddbj.nig.ac.jp/});
  \item строки Фибоначчи;
  \item произвольные строки над четырехбуквенным алфавитом.
\end{itemize}

Эти типы текстов были выбраны не случайно. Строки Фибоначчи являются одним из лучших примеров входных данных для задачи
построения ПП. На этом классе строк мы можем оценить практический потенциал ПП как модели сжатого представления.
Произвольные строки считаются не сжимаемыми и потенциально они являются наихудшим входом для задачи построения ПП.
Последовательности ДНК это класс хорошо сжимаемых строк, используемых на практике.

В работе сравниваются алгоритмы построения ПП с классическими алгоритмами сжатия из семейства Лемпеля-Зива. Наш тестовый
набор алгоритмов содержит классическую версию алгоритма Лемпеля-Зива из \cite{LZ77}. В дальнейшем для этого алгоритма
мы используем следующее обозначение: \textbf{lz77}. Размер окна сжатия равен 32Kb. Читателю может показаться, что размер
32Kb слишком мал для практических целей, но это значение было выбрано с целью подчеркнуть влияние размера окна на
степень сжатия.

Очевидно, размер окна сжатия влияет как на длины отдельных факторов, так и на размер факторизации в целом.
Поэтому если мы хотим получать более компактную факторизацию, то необходимо тратить больше ресурсов на
подготовительном этапе. Иначе говоря, размер окна сжатия напрямую влияет на размер факторизации. В тоже время не
существует прямой взаимосвязи между устройством ПП и размером текста выводимым из нее. Поэтому использование
компактных LZ-факторизаций влечет построение более компактных ПП. Следовательно, интересно рассмотреть версию алгоритма
Лемпеля-Зива с бесконечным окном сжатия. В дальнейшем для этого алгоритма мы используем следующее обозначение:
\textbf{lzma}.

Дополнительно тестовый набор алгоритмов содержит алгоритм Лемпеля-Зива-Велча (краткое обозначение \textbf{lzw}). 

Для удобства читателя были приняты общие обозначения для алгоритмов сжатия, которые представлены на рисунке
~\ref{algorithmNotations}.

\begin{figure}
	\caption{Обозначения алгоритмов сжатия}\label{algorithmNotations}
	\begin{center}
		\algorithmNotations
	\end{center}
\end{figure}

Эффективность алгоритмов оценивается относительно двух параметров: время работы и степень сжатия. Для алгоритмов
построения ПП с помощью AVL-деревьев дополнительно считается количество операций вращения. Степень сжатия определяется
как отношение между размером сжатого представления и размером исходного представления текста. Степень сжатия измеряется
в процентах. Например, формула для степени сжатия с помощью ПП выглядит следующим образом: $\frac{|\slp{S}|}{|S|} \cdot
100$. Поскольку отношение между бинарным представлением правила ПП и бинарным представлением LZ-фактора превосходит 1
(но фиксирован), то может возникнуть следующая ситуация: степень сжатия с помощью ПП меньше чем степень сжатия с помощью
алгоритма Лемпеля-Зива, но бинарное представление ПП превосходит бинарное представление LZ-словаря.

Все алгоритмы запускались в одинаковых условиях на компьютере со следующей спецификацией: процессов Intel(R) Core(TM)
i7-2600 CPU 3.4GHz, 8Гб оперативной памяти, ОС Windows 7 x64.

\subsection{Сравнительный анализ алгоритмов построения ПП}

В этом разделе представлен сравнительный анализ трех алгоритмов построения ПП. 

Как ожидалось, все алгоритмы работают чрезвычайно быстро на строках Фибоначчи и строят очень компактные ПП.
Например, 36-е слово Фиббоначчи размера порядка 40Мб обрабатывается в течение 1мс, а на выходе получается ПП размера около 100
правил. Этот факт доказывает, что существует класс строк, на котором модель сжатия с помощью ПП чрезвычайно эффективна.
В дальнейшем мы не будем обсуждать результаты полученные на строках Фибоначчи. 

Рисунок \ref{DNARotations} показывает как меняется количество операций вращения после применения оптимизации к алгоритму Риттера
\cite{SLPConstruction}. Даже для входных данных небольших размеров количество операций вращения уменьшилось около 10
раз.

\begin{figure}
	\caption{Статистика количества операций вращения AVL-дерева для ДНК)}\label{DNARotations}
	\begin{center}
		\DNARotations
	\end{center}
\end{figure}

Для того, чтобы оценить влияние операций вращения на скорость построения ПП были проведены два теста. В первом тесте алгоритмы
хранили построенное AVL-дерево в оперативной памяти, а во втором -- во внешнем файле (то есть каждое обращение к дереву
является обращением к файловой системе). Мы ожидаем, что при хранении ПП в оперативной памяти не будет принципиальной
разницы между алгоритмом Риттера и его оптимизированной версией, а алгоритм на основе декартовых деревьев будет быстрее.
Но как алгоритмы будут себя вести, если стоимость доступа к AVL-дереву будет дороже вычислений в оперативной памяти?
Такая ситуация естественным образом возникает в случае, когда размер входного текста очень велик и алгоритм не может
хранить все дерево в оперативной памяти. На рисунках \ref{DNATimeStatsInMemory} и \ref{DNATimeStatsOnFiles} представлены результаты
обоих тестов.

\begin{figure}
	\caption{Статистика времени работы алгоритмов построения ПП на ДНК (ПП хранится в
	оперативной памяти)}\label{DNATimeStatsInMemory}
	\begin{center}
		\DNATimeStatsInMemory
	\end{center}
\end{figure}

На рисунке \ref{DNAHeightStats} представлена статистика высот ПП полученных всеми алгоритмами на последовательностях
ДНК. Высоты ПП полученные с помощью классического алгоритма Риттера и с помощью его потимизированного варианта очень
близки, поэтому на рисунке предтавлены результаты только одного алгоритма. 

\begin{figure}
	\caption{Статистика времени работы алгоритмов построения ПП на ДНК (ПП хранится в
	отдельном файле)}\label{DNATimeStatsOnFiles}
	\begin{center}
		\DNATimeStatsOnFiles
	\end{center}
\end{figure}

\begin{figure}
	\caption{Статистика высот ПП на ДНК}\label{DNAHeightStats}
	\begin{center}
		\DNAHeightStats
	\end{center}
\end{figure}

Аналогичные эксперименты были проведены на произвольных текстах. Результаты этих исследований приведены на рисунках
\ref{RandomRotations}, \ref{RandomTimeStatsInMemory} и \ref{RandomTimeStatsOnFiles}. 

\begin{figure}
	\caption{Статистика количества операций вращения AVL-дерева для произвольных текстов)}\label{RandomRotations}
	\begin{center}
		\RandomRotations
	\end{center}
\end{figure}

\begin{figure}
	\caption{Статистика времени работы алгоритмов построения ПП на произвольных текстах (ПП хранится в
	оперативной памяти)}\label{RandomTimeStatsInMemory}
	\begin{center}
		\RandomTimeStatsInMemory
	\end{center}
\end{figure}

\begin{figure}
	\caption{Статистика времени работы алгоритмов построения ПП на произвольных текстах (ПП хранится в
	отдельном файле)}\label{RandomTimeStatsOnFiles}
	\begin{center}
		\RandomTimeStatsOnFiles
	\end{center}
\end{figure}

На рисунке \ref{RandomHeightStats} представлена статистика высот ПП полученных всеми алгоритмами на произвольных
текстах.

Результаты исследования показывают, что полученная оптимизированная версия алгоритма Риттера работает немного быстрее
при хранении ПП в оперативной памяти. А при хранении дерева в оперативной памяти тесты показывают, что новый алгоритм
работает в 2-3 раза быстрее чем классическая версия алгоритма Риттера. Таким образом оптимизированный алгоритм более
устойчив к росту входных данных. В тоже время алгоритм, использующий декартовы деревья, работает в 3-4 раза быстрее
чем алгоритмы, которые используют AVL-деревья. А высота декартова дерева в среднем в два раза больше чем высота
AVL-дерева.

\begin{figure}
	\caption{Статистика высот ПП на произвольных текстах}\label{RandomHeightStats}
	\begin{center}
		\RandomHeightStats
	\end{center}
\end{figure}

\subsection{Сравнительный анализ степени сжатия}

Рисунки \ref{CompressionRatioOnDNAs} и \ref{CompressionRatioOnRandoms} показывают, что классический алгоритм Риттера и
его оптимизированная версия обеспечивают очень степень сжатия близкую к степени сжатия классических алгоритмов (LZ77 и
LZW). Как и ожидалось алгоритм на декартовых деревьях обеспечивает более плохую степень сжатия. При этом алгоритмы
сжатия на основе AVL-деревьев обеспечивают степень сжатия, которая не более чем в два раза хуже алгоритмов LZ77 и LZW.
А в результате алгоритмы сжатия на основе ПП строят хорошо-структурированное представление данных, которое поддерживает
поисковые запросы.

\begin{figure}
	\caption{Статистика степени сжатия на ДНК}\label{CompressionRatioOnDNAs}
	\begin{center}
		\resizebox{10cm}{12cm}{%
	    	\CompressionRatioOnDNAs
		}
	\end{center}
\end{figure}

\begin{figure}
	\caption{Статистика степени сжатия на произвольных текстах}\label{CompressionRatioOnRandoms}
	\begin{center}
		\resizebox{10cm}{12cm}{%
	    	\CompressionRatioOnRandoms
		}
	\end{center}
\end{figure}

\section{Архитектура проекта}

Код проекта находится в свободном доступе. Скачать исходный код проекта можно по адресу
\url{http://overclocking.googlecode.com/svn/webService/trunk/}. 

В проекте были использованы следующие внешние библиотеки:

\begin{itemize}
  \item \textbf{log4j} (\url{http://logging.apache.org/}) -- библиотека для логирования событий, происходящих в системе;
  \item \textbf{simple xml serializer} (\url{http://simple.sourceforge.net/}) -- библиотека для
  сериализации/десериализации объектов в формате xml;
  \item \textbf{container} (\url{http://overclocking.googlecode.com/svn/webService/trunk/IDEContainer/}) -- реализация
  DI контейнера;
  \item \textbf{cassandra} (\url{http://cassandra.apache.org/}) -- распределенная файловая система;
  \item \textbf{hector} (\url{https://github.com/rantav/hector}) -- клиент для распределенной файловой системы;
\end{itemize}

Архитектура проекта состоит из следующих компонентов:

\begin{enumerate}
  \item Кластер распределенной файловой системы cassandra. Кластер состоит из трех узлов и обеспечивает
  отказоустойчивость и доступность данных;
  \item Набор сервисов сжатия данных. Для каждого алгоритма сжатия данных написан свой сервис. Каждый сервис сжатия
  данных отслеживает изменения в распределенной файловой системе. А именно, сервис отслеживает появление новых текстов в
  системе. Как только появляется новый текст запускается задача по сжатию этого текста. В процессе сжатия текста
  собирается статистика о результатах сжатия данных. Далее полученная статистика сохраняется в распределенной файловой 
  системе.
  \item Сервис публикации данных -- сервис, который публикует свежую статистику в открытый доступ. А именно это сервис,
  который интегрируется с сайтом, на котором опубликована статистика по сжатию данных. Сайт расположен по адресу
  \url{http://overclocking.usu.edu.ru}.
\end{enumerate}

\subsection{Реализация алгоритмов сжатия}

В этом разделе мы представим основные трудности с которыми пришлось столкнуться во время реализации алгоритмов сжатия. 

Основным объектом с которым работают алгоритмы сжатия на основе ПП является факторизация текста. Но как получать эту
факторизацию? Существует большое число способов разбить текст на части. В проекте реализованы два способа факторизации
текста. Обе факторизации соответствуют естественному разбиению текста получаемому алгоритмами сжатия LZ77 и LZMA.
Реализация алгоритмов факторизации различна, так как факторизация на основе LZ77 в каждый момент времени просматривает
ограниченый участок текста, в то время как LZMA просматривает весь доступный текст. 

В процессе факторизации необходимо быстро отвечать на поисковые запросы, а именно, содержится ли некоторая подстрока
в рабочей области. Для решения таких задач существуют две похожие структуры данных: суффиксный массив и суффиксное
дерево. Обе структуры данных поддерживают поисковые запросы, но отличаются по способу хранения данных. Так каждый узел
суффиксного дерева хранит ссылки на своих потомков, следовательно дополнительно расходуется память на хранение ссылок.
Более того, если память под данные выделяется каким-то универсальным механизмом, то потенциально ссылки на потомком
могут быть раскиданы произвольно по оперативной памяти. Следовательно, при работе с деревом возникнет большее
количество переходов в памяти, что негативно сказывается на работе кэша памяти. Так же потенциально число потомков
некоторого узла равно размеру алфавита, следовательно надо организовывать работу с большим числом потомком. А это также
требует ресурсы оперативной памяти. С другой стороны работа с суффиксным деревом более удобна. Из представленных
аргументов следует, что суффиксные деревья больше подходят для решения небольших (локальных) задач.

Поскольку алгоритм LZ77 использует рабочее окно фиксированного размера, то для построения факторизации используются
суффиксные деревья. Алгоритм построения LZ77-факторизации состоит из следующих шагов:

\begin{itemize}
  \item Один раз алгоритм строит суффиксное дерево на основе рабочей области;
  \item На основе запросов к суффиксному дереву алгоритм находит максимальный фактор, который уже содержится в рабочей
  области;
  \item После этого сдвигается рабочая область и соответственно перестраивается суффиксный массив. Необходимо
  подчеркнуть, что суффиксный массив не строится заново, а именно локально модифицируется. За счет локальной модификации
  дерева достигается большая эффективность алгоритма.
\end{itemize}

Поскольку алгоритм LZMA не накладывает никаких ограничений на рабочую область и размеры входа могут быть достаточно
большими, то для построения LZMA-факторизации более удобен суффиксный массив. Алгоритм построения LZMA-факторизации
содержит следующие шаги:

\begin{itemize}
  \item Алгоритм строит один суффиксный массив для всей входной строки;
  \item Последовательно просматривая входную строку слева направо алгоритм разбивает строку на факторы, обращаясь при
  этом с запросами к суффиксному массиву;
\end{itemize}

При реализации алгоритмов сжатия на основе ПП мы столкнулись с тем, что алгоритмы построения больших деревьев очень
ресурсоемки. Например, если размер исходного текста равен 10 Мб, то алгоритм сжатия на основе ПП может потребовать
порядка 3-4 Гб оперативной памяти. Такое поведение связано с постоянным перестроением дерева и поддержанием его
сбаланированности во время обработки очередного фактора. А именно, в процессе перебалансировки дерева могут порождаться
и новые узлы, и узлы на которые больше никто не ссылается. Так как скорость построения дерева велика, то
рассчитывать, что на то, что сборщик мусора (garbage collector) соберет неиспользуемые ссылки не приходится. Поэтому все
алгоритмы сжатия содержат модуль управления памятью. Более того, такой модуль необходим если алгоритм сжатия хранит
сжатое представление сразу в файловой системе. 

Для каждого узла дерева алгоритм дополнительно хранит количество внешних ссылок на этот узел. При выполнения операций,
которые модифицируют дерево алгоритм изменяет число ссылок на правило. Если число ссылок на узел стало равно нулю, то
он помечается как свободный. Другими словами мы не удаляем правила, которые были хотя бы раз использованы, а помечаем их
как правила, которые можно полностью менять и при этом не испортится уже построенное дерево. 

После реализации представленной выше модели управления узлами потребности алгоритмов сжатия в оперативной памяти
сократились от трех до восьми раз. 

\section{Методология разработки}

В нашем проекте для ведения разработки используется методология Agile. Agile это методология разработки, которые
основываются на последовательном (итеративном) ведении процесса разработки. Одной из целей этой методологии является
минимизация рисков проекта за счет регулярного получения обратной связи и пересмотра планов. В рамках этой методологии
сформулированы различные техники, которые позволяют достичь желаемого результата. Понятно, что в зависимости от проекта
и от команды, которая разрабатывает проект, какие-то техники могут быть более эффективны, а какие-то менее. Нашему
проекту больше подходят техники экстремального программирования, которые сформулированы в \cite{Beck}, \cite{Fowler} и
\cite{Miller}.

В рамках нашего проекта актуальность техник экстремального программирования сложно недооценить. Потому, что допущенная
на раннем этапе ошибка будет очень дорого стоить на этапе использования. Например, если запущен алгоритм сжатия данных,
который хранит сжатое представление в файловой системе, то он может работать в течение нескольких часов и при этом будут
записаны мегабайты логов. Пусть перед самым концом алгоритм завершился исключением, тогда уже потеряно несколько часов
работы зря, получено большое количество логов, которые необходимо анализировать, чтобы понять причину ошибки. Техники
экстремального программирования позволяют свести вероятность такой ситуации к минимуму. Существуют различные техники
экстремального программирования ( > 10), в нашей команде используются следующие:

\begin{enumerate}
  \item разработка через тестирование;
  \item игра в планирование;
  \item парное программирование;
  \item непрерывная интеграция;
  \item рефакторинг;
  \item частые релизы;
  \item простой дизайн;
  \item коллективное владение кодом;
  \item стандарт кодирования;
\end{enumerate}

Рассмотрим каждую технику в отдельности, чтобы понять какую цель преследует каждая из техника.

\textbf{Разработка через тестирование} 

Выделяются следующие виды тестов:

\begin{itemize}
  \item модульные тесты (фиксируют взаимодействие отдельного модуля (класса) с внешним миром):
  \item интеграционные (фиксируют базовые сценарии поведения некоторого блока, например, интеграционные тесты на
  взаимодействие с базой данных);
  \item функциональные (фиксируют работу функционала из пользовательского интерфейса);
  \item приемочные (фиксируют работоспособность бизнес процессов);
\end{itemize} 

Поскольку пользовательский интерфейс проекта еще достаточно невелик, то в проекте используются только модульные и
интеграционные тесты. Что дает проекту тестирование? 

Разработчик не может быть уверен в правильности написанного им кода до тех пор, пока не сработают абсолютно все тесты.
Модульные тесты позволяют разработчикам за очень короткое время убедиться в том, что их код работает корректно. Модульные
позволяют разработчику без каких-либо опасений выполнять рефакторинг проекта. Также они помогают другим разработчикам
понять, зачем нужен тот или иной фрагмент кода и как он функционирует. Фактически модульные тексты можно использовать в
качестве проектной документации.

Интеграционные тесты гарантируют, что довольно большие участки проекта работоспособны. А если интеграционный тест
находит ошибку, то эта ошибка локальна и не надо тратить время на то, чтобы определить в каком блоке проекта
произошел сбой.

В рамках экстремального программирования используется подход, называемый Test Driven Development -- сначала
пишется тест, который не проходит, затем пишется код, чтобы тест прошел, а в заключении делается рефакторинг кода.

\textbf{Игра в планирование} 

Основная цель игры в планирование -- сформировать приблизительный план работы на ближайшую итерацию и постоянно
обновлять его по мере того, как условия задачи становятся все более четкими. В этой игре участники делятся на две
команды. В первой команде находятся члены команды разработки, а во второй представители внешнего мира (сотрудники,
которые исследуют потребности клиентов). Вначале представители внешнего мира формируют набор задач, который упорядочен
по степени важности с точки зрения клиента. Потом участники второй команды подробно рассказывают о каждой задаче. А
участники первой команды в процессе обсуждения формируют стоимость задачи. Если задача оказывается слишком дорогой
участники второй команды могут убрать задачу из плана. Критическим фактором, благодаря которому такой стиль
планирования оказывается эффективным, является то, что в данном случае внешний мир отвечает за принятие бизнес-решений,
а команда разработчиков отвечает за принятие технических решений. 

\textbf{Парное программирование}

Парное программирование предполагает, что большая часть кода создается парами программистов, работающих за одним
компьютером. Один из них работает непосредственно с текстом программы, другой просматривает его работу и следит за общей картиной
происходящего. При необходимости клавиатура свободно передается от одного к другому. В течение работы над проектом пары
не фиксированы: рекомендуется их перемешивать, чтобы каждый программист в команде имел хорошее представление о всей
системе. Таким образом, парное программирование усиливает взаимодействие внутри команды, улучшает качество решения
задачи, способствует обмену знаниями внутри команды.

\textbf{Непрерывная интеграция}

Если выполнять интеграцию разрабатываемой системы достаточно часто, то можно избежать большого количества проблем. В
традиционных методиках интеграция, как правило, выполняется в самом конце работы над продуктом, когда считается, что
все составные части разрабатываемой системы полностью готовы. В экстремального программирования интеграция кода всей
системы выполняется несколько раз в день, после того, как разработчики убедились в том, что все модульные тесты
корректно работают. Это практика позволяет в очень сжатые сроки собрать работающую версию продукта. Например, это
актуально, когда необходимо выложить обновление проекта.

\textbf{Рефакторинг}

Рефакторинг —- это методика улучшения кода, без изменения его функциональности. Предполагается, что однажды написанный
код в процессе работы над проектом почти наверняка будет неоднократно переделан. Разработчики переделывают написанный
ранее код для того, чтобы улучшить архитектуру проекта и снизить стоимость поддержки продукта. Этот процесс называется
рефакторингом. Отсутствие тестового покрытия провоцирует отказ от рефакторинга, в связи с боязнью сломать функционал.
Результатом отказа от рефакторинга является постепенная деградация кода.

\textbf{Частые релизы}

Версии продукта должны поступать в эксплуатацию как можно чаще. Работа над каждой версией должна занимать
как можно меньше времени. При этом каждая версия должна быть достаточно осмысленной с точки зрения полезности для
внешнего мира.

Чем раньше мы выпустим первую рабочую версию продукта, тем раньше клиент начнет получать необходимую функциональность.
Чем раньше клиент приступит к эксплуатации продукта, тем раньше разработчики получат от него информацию о том, что
соответствует требованиям заказчика, а что — нет. Эта информация может оказаться чрезвычайно полезной при планировании
следующего выпуска.

\textbf{Простота дизайна}

В концепции экстремального программирования предполагается, что в процессе работы условия задачи могут
неоднократно измениться, а значит, разрабатываемый продукт не следует проектировать заблаговременно целиком и
полностью. Если в самом начале работы вы пытаетесь от начала и до конца детально спроектировать систему, вы напрасно
тратите время. Проектирование —- это настолько важный процесс, что его необходимо выполнять постоянно в течение всего
времени работы над проектом. Проектирование должно выполняться небольшими этапами, с учетом постоянно изменяющихся
требований. В каждый момент времени мы пытаемся использовать наиболее простой дизайн, который подходит для решения
текущей задачи. При этом мы меняем его по мере того как условия задачи меняются.

\textbf{Стандарты кодирования}

Все члены команды в ходе работы должны соблюдать требования общих стандартов кодирования. Благодаря этому члены команды
не тратят время на споры о вещах, которые фактически никак не влияют на скорость работы над проектом и обеспечивается
эффективное выполнение остальных практик.

Если в команде не используются единые стандарты кодирования, разработчикам становится сложнее выполнять рефакторинг.
При смене партнеров в парах возникает больше затруднений. Следовательно, продвижение проекта затрудняется.
Поэтому необходимо добиться того, чтобы было сложно понять, кто является автором того или иного участка кода, —- вся
команда работает унифицировано, как один человек. Команда должна сформировать набор правил, а затем каждый член
команды должен следовать этим правилам в процессе кодирования. Перечень правил не должен быть исчерпывающим или слишком
объемным. Задача состоит в том, чтобы сформулировать общие указания, благодаря которым код станет понятным для каждого
из членов команды. Стандарт кодирования поначалу должен быть простым, затем он будет эволюционировать по мере того, как
команда обретает опыт. Не следует тратить на предварительную разработку стандарта слишком много времени.

\textbf{Коллективное владение}

Коллективное владение означает, что каждый член команды несёт ответственность за весь исходный код. Таким образом,
каждый вправе вносить изменения в любой участок программы. Парное программирование поддерживает эту практику: работая в
разных парах, все программисты знакомятся со всеми частями кода системы. Важное преимущество коллективного владения
кодом заключается в том, что оно ускоряет процесс разработки, поскольку при появлении ошибки её может устранить любой
программист.

Давая каждому программисту право изменять код, мы получаем риск появления ошибок, вносимых программистами, которые
считают что знают что делают, но не рассматривают некоторые зависимости. Хорошо определённые модульные тесты решают эту
проблему: если нерассмотренные зависимости порождают ошибки, то следующий запуск модульных тестов будет неудачным.

\backmatter

\Conclusion

В работе представлены два новых алгоритма сжатия на основе ПП. Первый алгоритм очень похож на классический
алгоритм Риттера из \cite{SLPConstruction}, но за счет оптимизации более устойчив к росту входных данных. Второй
алгоритм использует более простую (с точки зрения построения) структуру дерева и поэтому более эффективен по времени
построения. Алгоритм на основе декартовых деревьев строит более высокие ПП, чем алгоритмы на основе AVL-деревьев.

В работе представлен сравнительный анализ алгоритмов сжатия на основе ПП и классических алгоритмов сжатия. Результаты
показывают, что алгоритмы построения ПП ресурсоемки и уступают классическим алгоритмам по времени сжатия. С другой
стороны степень сжатия, достигаемая алгоритмами на основе ПП, близка к степени сжатия, достигаемой алгоритмами из
семейства Лемпеля-Зива. Также высота полученного сжатого представления получается небольшой. 

Полученные алгоритмы сжатия могут быть использованы на практике, но для различных целей. Сравнительный анализ
показал, что оптимизированный алгоритм Риттера больше подходит для построения компактных ПП, рассчитанных на дальнейший
поиск. А алгоритм на основе декартовых деревьев больше подходит для практических нужд, так как демонстрирует
высокую скорость работы. Нам кажется, что алгоритмически ускорить построение ПП очень трудно. Однако, на данный момент
алгоритм сжатия на основе декартовых деревьев показывает очень хорошую скорость, а вычислительная техника становится
мощнее с каждым годом. Больший интерес представляет вопрос о том насколько большой сжимающий потенциал у ПП. В данной
работе представлена эвристика на основе которой строится ПП, но это не значит что не существует другой ПП меньшего
размера, выводящий исходный текст. Также интересно сравнить эффективность поисковых алгоритмов на ПП и на строках. А
именно, на каких объемах входных данных алгоритмы на ПП смогут обогнать классические строковые алгоритмы.

Статья, содержащая улучшенный алгоритм Риттера, была представлена на международной конференции 1st International
Conference on Data Compression, Communication and Processing \url{http://ccp2011.dia.unisa.it/CCP_2011/Home.html} и
принята к печати в сборник работ коференции. Статья, содержащая алгоритм построения ПП на основе декартовых деревьев,
была отправлена на рецензию в журнал Записки научных семинаров ПОМИ \url{http://www.pdmi.ras.ru/znsl/}.

\bibliography{references}

\end{document}
